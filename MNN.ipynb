{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HExGAmdDalm0"
   },
   "source": [
    "# Multilayer Perceptron in Python\n",
    "## COMP5329 Assignment 1\n",
    "\n",
    "## Team:\n",
    "Anthony Moriarty, 460285074\n",
    "\n",
    "Ben Daly, 480321620\n",
    "\n",
    "Louis Rankin, 430367324\n",
    "\n",
    "##1.0 Introduction\n",
    "The experiment task consisted of building a neural network to perform multi-class classification on a supplied dataset without the use of Deep Learning frameworks (e.g. TensorFlow, Caffe, and KERAS). The dataset consisted of 60,000 labeled training samples and 10,000 unlabeled test samples. The structure of the data (e.g. image, video, etc) was unknown. The performance of the neural network was evaluated in terms of the accuracy metric. Various neural network structures and parameters were trailed to maximise speed and accuracy.\n",
    "\n",
    "The objective of building the neural network without Deep Learning frameworks was to gain a comprehensive understanding of the math and mechanics behind neural networks. This is important due to the increasing number of fields where neural networks are the state-of-the-art solution for machine learning.\n",
    "\n",
    "##2.0 Methods\n",
    "###2.1 Equipment\n",
    "The programing language Python3 was used to build the neural network. A Google colaboratory image with the following specifications was used to run the neural network:\n",
    "*   Processor:  Intel(R) Xeon(R) CPU @ 2.30GHz - 1 core\n",
    "*   Memory: 12 GB\n",
    "*   Operating system: Linux\n",
    "\n",
    "###2.2 Preprocessing\n",
    "\n",
    "Preprocessing data before feeding it to a Neural Network is important for many reasons. Firstly, greatly differing magnitudes between different features can lead to the network assigning false importance to the larger features. Preprocessing also speeds up convergence - centering the data can mitigate the vanishing gradient problem on activation functions such as sigmoid and tanh, while normalization can mitigate the exploding gradient problem with ReLU activation passing through huge net values.\n",
    "\n",
    "The method selected was z-score normalization. This is a two-step process - run on each feature independetly. First the mean of each feature is subtracted to centre it on zero; secondly the standard deviation is calculated and the feature divided by the standard deviation. The end result is features with zero mean and standard deviation 1. This ensures most of the variance is within the -1 to 1 range where most activation functions have the largest gradient.\n",
    "\n",
    "The output data was also one-hot encoded to create one column per output class. This maps back to the output of a neural network with one node per output class.\n",
    "\n",
    "###2.3 Weight Initialization\n",
    "\n",
    "The goal of weight initialization is to initialize the layers of the network with small random numbers to ensure that different nodes learn different things. We want to preserve variance through the network, so we examine each layer independently with the goal that outputs of a layer should have the same variance as the inputs. In order to preserve this variance for different activation functions, two methods of weight initialization are employed.\n",
    "\n",
    "Both methods are used to calculate the parameters of a distribution from which weights are selected. Two options are presented to the user - a uniform distribution and a truncated normal distribution. A truncated normal distribution is selected to remove large outliers - any weight with absolute value larger than the maximum of 2 standard deviations has its absolute value set to this limit.  \n",
    "\n",
    "###2.3.1 Xavier Initialization\n",
    "\n",
    "For sigmoid and tanh activation functions, the preservation of variance depends on both the number of inputs and the number of outputs of the layer. Firstly, for the truncated normal case, we want to average the number of inputs and outputs to select parameters for our distribution which are optimal for both.\n",
    "\n",
    "$$Var(W) = \\frac{1}{\\frac{n_{in} + n_{out}}{2}} = \\frac{2}{n_{in} + n_{out}} $$                 \n",
    "$$\\sigma(W) = \\sqrt{\\frac{2}{n_{in} + n_{out}}} $$\n",
    "\n",
    "To select bounds for a uniform distribution, we apply the function\n",
    "$$Var(\\mathit{Uniform(-limit, limit)}) = \\frac{limit^2}{3} $$\n",
    "$$Var(W) = \\frac{2}{n_{in} + n_{out}}  \\Leftrightarrow  limit = \\sqrt{\\frac{6}{n_{in}+n_{out}}} $$\n",
    "\n",
    "\n",
    "###2.3.2 He Initialization\n",
    "\n",
    "For ReLU activation, we encounter the issue that a ReLU unit halves the variance - the variance of negative numbers is nullified. So we must double the above formulae to compensate. One further simplification is that in their original paper, *He. et al* demonstrated that their initialization scheme works for both forward and backward passes, meaning we only need to consider $n_{in}$ and not the average of both. For truncated normal we derive the bounds:\n",
    "$$ Var(W) = \\frac{2}{n_{in}} $$\n",
    "\n",
    "$$ \\sigma(W) = \\sqrt{\\frac{2}{n_{in}}} $$\n",
    "\n",
    "Applying the function for the variance of limits from the Xavier Initialization section, we derive the bounds for uniform He initialization:\n",
    "\n",
    "$$ limit = \\sqrt{\\frac{6}{n_{in}}} $$\n",
    "\n",
    "##2.4 Training\n",
    "\n",
    "### 2.4.1 Gradient Descent\n",
    "Gradient descent is a machine learning optimization method used in deep learning to calculate the model parameters (weights and biases) that minimise the cost function. The gradient descent method invovles iterating through a training dataset and updating weights and baises in accordance with the gradient of error. There are three types of gradient descent. Each uses a different number of training examples to update the model parameters:\n",
    "*   **Batch Gradient Descent** uses the entire training dataset to calculate gradients and update the parameters. Because the entire training dataset is considered parameters updates are smooth however, it can take a long time to make a single update.\n",
    "*   **Stochastic Gradient Descent (SGD)** uses a single randomly selected sample from the training dataset to calculate gradients and update the parameters. Parameter updates are fast but very noisey.\n",
    "*   **Mini-batch Gradient Descent** uses a subset of the training data (e.g. batches of 1000 samples) to calculate gradients and update the parameters. Mini-batch gradient descent is a compromise between batch and stochastic gradient descent. The mini-batch size can be adjusted to find the appropriate balance between fast convergence and noisey updates. \n",
    "\n",
    "Mini-batch gradient descent was inplemented in the neural network due to the flexibility provided by the adjustable mini-batch size. Mini-batch sizes between 32 and 512 samples were used in accordance with the findings of *Keskar et al, 2017.* *Keskar et al* observed that larger batch-sizes resulted in a degradation in the quality of the model, as measured by its ability to generalize. A hybrid of batch gradient descent and stocahastic gradient descent was also implemented. In this method gradients were updated using the same number of samples as the entire training dataset (i.e. 60,000 samples). However, the 60,000 samples were draw stochastically meaning that some sample would have been used multiple times and others not at all. The stochastic approach to batch gradient ensured the neural network was fed data in a different sequence each epoch. This aids the neural networks ability to generalise.\n",
    "\n",
    "### 2.4.2 Stochastic Gradient Descent (SGD) with Momentum\n",
    "Momentum ($v_t$) is an exponentially weighted average of a neural networks gradients. It was used to update the weights ($w_t$) and biases ($b_t$) of the network.\n",
    "\n",
    "$$v_t = \\beta v_{t-1} + \\eta \\nabla_w J(w)$$\n",
    "$$w_t = w_{t-1} - v_t$$\n",
    "\n",
    "Momentum increases for features whose gradients point in the same direction and reduces for features whose gradients change direction. By reducing the fluctuation of gradients convergence is generally sped up. The hyper-parameter $\\beta$ takes a value between 0 - 1 and dictates how many samples are included in the exponential weighted average. A small $\\beta$ value will increase fluctuation because the average is taken over a smaller number of examples. A large $\\beta$ will increase smoothing because the average is taken over a larger number of examples. A $\\beta$ value of 0.9 was implemented. 0.9 is a widely used value for $\\beta$ (*Ruder, 2016*) as it provides a balance between the two extremes of over-smoothing and excessive fluctuations. \n",
    "\n",
    "\n",
    "### 2.4.3 Batch Normalization\n",
    "Batch Normalization is the normalization of a neural network's hidden layers so that the hidden units have standardised mean and variance. It is carried out on training data mini-batches, typically before the activation function is applied *(Ioffe et al, 2015)*. For each mini-batch $(MB)$ the mean $(\\mu)$ and variance $(\\sigma^2)$ is calculated for all features:\n",
    "\n",
    "$$\\mu_{MB} = \\frac{1}{m} \\sum_{i = 1}^{m} x_i$$\n",
    "$$\\sigma_{MB}^2 \\frac{1}{m} \\sum_{i = 1}^{m} (x_i - \\mu_{MB})^2 $$\n",
    "\n",
    "The normalized values of the hidden unit $x_i$ can then be calculated:\n",
    "\n",
    "$$\\tilde{x_i} = \\frac{x_i - \\mu_{MB}}{\\sqrt{\\sigma_{MB}^2+\\epsilon}}$$\n",
    "\n",
    "$\\tilde{x_i}$ has a mean and variance of 0 and 1 respectively. It may be advantageous to alter the mean and variance of $\\tilde{x_i}$ to manipulate its distribution. Learnable hyperparameters $\\gamma$ and $\\beta$ are introduced to $\\tilde{x_i}$ for this purpose:\n",
    "$$\\tilde{x_{i}} = \\gamma\\tilde{x_i} + \\beta $$\n",
    "\n",
    "While debate remains as to why batch normalization is effective *Ioffe et al* offer two reasons why batch normalization speeds up training. Firstly, as reported above, normalizing the input features of a neural network can speed up learning. This is becuase the cost function becomes more symetrical and larger steps (i.e. larger learning rate) can be taken. The same holds true when the hidden unit values are normalized. The second reason is that batch normalization makes weights deeper in the network more robust to changes that take place in earlier layers. During training the distribution of each layers inputs are adjusted as the parameters of the previous layer change. This is known as internal covariate shift. Batch normalization reduces the amount of covariate shift that occurs throughout the networks layers. This is particuarly benefical for the later layers of the network becausethe earlier layers don't shift as much due to their constrained mean and variance. Batch normalisation therefore allows for larger learning rates. It also means that one can be less careful with the initilisation of weights and bias. A secondary benefit of batch normalization is regularization which helps with model overfit.\n",
    "\n",
    "##2.5 Regularization\n",
    "\n",
    "A common issue in machine learning is the tendency to overfit - a model which perfectly describes the training data may fail to generalize when it comes to unseen data. Various methods can be used to regularize the weights of a neural network to ameliorate overfitting.\n",
    "\n",
    "### 2.5.1 Early Stopping\n",
    "\n",
    "The simplest method is an attempt to reduce over-training - early stopping. To achieve this, we split the data into a large training set, and a small validation set. The accuracy on the held-out validation set is measured at the end of each epoch and stored. If the accuracy has not improved for a certain number of epochs, we cease training and return the model.\n",
    "\n",
    "### 2.5.2 Weight Decay\n",
    "\n",
    "Weight decay is essentially a form of $l2$ regularization. An additional term is added to the loss function to suppress larger weights. Used along with back-propagation, this additional term will see the weights tend to decay toward zero when they would otherwise be untouched. \n",
    "\n",
    "The following term is added to the loss function:\n",
    "$$ \\lambda \\sum W_{i}^2 $$\n",
    "\n",
    "During back-propagation, this translates to an additional gradient on each weight of \n",
    "\n",
    "$$ 2W * \\lambda $$\n",
    "\n",
    "where $ \\lambda $ is an additional parameter of the model - the regularization weight.\n",
    "\n",
    "### 2.5.3 Dropout\n",
    "\n",
    "Dropout is a regularisation method patented by Google. It consists of setting the output of neurons in hidden layers to 0 based on a binomial distribution where the probaibility is set as a hyperparameter of the model. The remaining weights are then scaled inversely proportionally to the probaibility set. This is to ensure that the expected magnitude of the neural networks output remains the same as when dropout is not applied (Like during prediction time). \n",
    "\n",
    "It can be said that all neural networks use dropout, however most of them apply a probability of 1 effectivly causing no change to the output. This is what was implemented for this assignment with the default probability being set to 1. \n",
    "\n",
    "Dropout has also been described as a way of performing model averaging without explicitly training different models like what is done with boosting. This description is used because of any given training example a small subset of the neural network learns how to fit the example. This creates a weak learner. Since the subsets of the neural network all share their weights the result is similar to averaging all of their outputs. More on this idea can be found here *(Baldi et al. 2013)*\n",
    "\n",
    "##2.6 Activation Functions\n",
    "\n",
    "Two different activation functions we're implemented as part of this neural network. The Sigmoid and Tanh activation functions were supplied.\n",
    "\n",
    "### 2.6.1 ReLU\n",
    "\n",
    "The ReLU also known as Rectified Linear Unit formulation:\n",
    "\n",
    "$ f(x) = \\begin{cases}\n",
    "    x & \\mbox{if } x > 0 \\\\\n",
    "    0 & \\mbox{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "The ReLU activation function has gained some popularity in part due to the properties of it's gradient. It's derivative is very quick to calculate and does not get extremely small or large. This is because the formulation for its derivative is: \n",
    "\n",
    "$ f'(x) = \\begin{cases}\n",
    "    1 & \\mbox{if } x > 0 \\\\\n",
    "    0 & \\mbox{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "\n",
    "A variant of ReLU was also implemented. This variant is called Leaky ReLU. The leaky component of ReLU is because it still retains some information when the input value is less than 0 as described below:\n",
    "\n",
    "$ f(x) = \\begin{cases}\n",
    "    x & \\mbox{if } x > 0 \\\\\n",
    "    0.01x & \\mbox{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "The derivative for this retains most of the positive properties that the standard ReLU has but this Leaky version results in no dead neurons. This is when a neuron only produces negative values and therefor will only output 0. This means that neurons derivative will also be 0 and it's weights will never change. More on this can be found here *(Xu et al. 2015*)\n",
    "\n",
    "$ f'(x) = \\begin{cases}\n",
    "    1 & \\mbox{if } x > 0 \\\\\n",
    "    0.01 & \\mbox{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "### 2.6.2 Softmax\n",
    "The Softmax activation function is primarily used on the last layer of a neural network so that its outputs resemble a probability distribution. It achieves this by processing a whole layer as a group instead of individual neurons like the other activation functions used. It does this by rescaling or normalising the outputs of the layer to between 0 and 1. It does this to the eponential of all the values. This is because the result is always postive and the result is closer to the result from argmax than regular normalisation is. This is helpful because during prediction time argmax is used but this is not differentiable and so cannot be used in training. \n",
    "\n",
    "The specific implementation of softmax used is from the SciPy library. The source code for this can be found here *(SciPy. 2018)*\n",
    "\n",
    "This particular implementation was used as everything is computed in log space which leads to better numerical stability. \n",
    "\n",
    "##2.7 Cost Function\n",
    "\n",
    "### 2.7.1 Cross Entropy Loss\n",
    "Cross entropy loss is a cost function that is often used with the Softmax activation function. This is because cross entropy is a measure of the difference between two probability distributions. It can be superior to Mean Square Error on this kind of prediction because it only considers the distance between the predicted probability of the label class and its probability. This can have some computational advantages. This can be seen in the implementation of Cross Entropy Loss used which only considers 1 of the predicted probabilities. \n",
    "\n",
    "\n",
    "The properties of it's derivative are also desirable over Mean Square Error. This is because for errors that are very large or very small Mean Square Error can produce very small gradients while Cross Entropy loss when used with Softmax produces larger gradients during this time which can help with convergence. To read more about the advantages of Cross Entropy Loss see *(Kline et al, 2005)*\n",
    "\n",
    "# 3.0 Analysis\n",
    "\n",
    "## 3.1 Hyperparameters Search\n",
    "A random grid search was employed to find the combination of hyper-parameters resulting in the highest accuracy. *Bergstra et al, 2012* showed empirically and theoretically that randomly selected hyper-parameters are more efficient for optimization than a uniform grid search. The hyper-parameters that were searched were:\n",
    "*   Initialisation (uniform or normal)\n",
    "*   Dropout (0.2 , 0.4, 0.6, 0.8, 1.0)\n",
    "*   Activation function (reLu + Softmax or Sigmoid + Tanh)\n",
    "*   Weight decay (0, 0.05, 0.1, 0.2)\n",
    "*   Learning rate (0.005, 0.001, 0.1, 0.5)\n",
    "\n",
    "When a network failed to converge hyper-parameters were adjusted manually in an effort to better understand why. When interesting or unexpected results were observed manual adjustment of hyper-parameters was again carried out to aid understanding.\n",
    "\n",
    "####Table 1: Random grid search results for two layer network \n",
    "Experiment number     |    1 |        2   |   3    |     4     |  5|6    |   7   | 8      |  9  \n",
    ":--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :---\n",
    "Number hidden layers      |   2     |    2      |   2   |      2    |     2|2     |    2    |     2     |    2 \n",
    "Hidden layer node dimensions|69 : 99|69 : 99|69 : 99|69 : 99|69 : 99|69 : 99|69 : 99|69 : 99|69 : 99\n",
    "Initialisation   |    uni    |  norm   |   norm   |   norm    |  norm   |norm    |   uni    |   uni    |   uni \n",
    "Dropout    |   0.6   |    0.6    |     1    |     1    |     1   |1    |   0.4    |   0.2  |     0.6 \n",
    "Activation functions  | reLU+SM|  Log+Tanh|  Log+Tanh  | reLU+SM |  reLU+SM   |reLU+SM | Log+Tanh | Log+Tanh | Log+Tanh \n",
    "Batch normalization     |    n   |      n   |      n    |     n       |  n   |n      |   n   |      n   |      n \n",
    " Weight Decay    |     0   |   0.05    |  0.05  |    0.05   |   0.05  | 0.05     |    0    |  0.05   |   0.05 \n",
    "Learning Rate     |  0.5   |   0.01  |    0.01   |    0.1   |   0.01   |0.01  |    0.01 |      0.1    |   0.5 \n",
    "Early Stopping      |   Y    |     Y   |      Y      |   Y    |     Y  | Y     |    Y    |     Y      |   Y \n",
    "Epochs   |    500     |  500  |     500    |   500    |   500   |500      | 500   |    500    |   500\n",
    "Run time (s)    | 49.42  |  281.81  |  281.49   | 436.56  |  803.74   |43.78   |  43.31   |  51.09   |  49.94 \n",
    "Accuracy | 10.0000% | 10.0000% | 88.4333% | 10.0000% | 86.8556%|87.4556% | 10.0000%  |10.0000% | 10.0000%\n",
    "\\\\\n",
    "\n",
    "Experiment number |10|\t11\t|12\t|13\t|14\t|15\t|16\t|17\t|18\t\n",
    ":--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- \n",
    "Number hidden layers      | 2|\t2|\t2|\t2|\t2|\t2|\t2|\t2|\t2\n",
    "Hidden layer node dimensions|69 : 99|69 : 99|69 : 99|69 : 99|69 : 99|69 : 99|69 : 99|69 : 99|69 : 99\n",
    "Initialisation   | uni\t|uni\t|norm|\tuni|\tuni|\tuni|\tuni|\tnorm|\tnorm\n",
    "Dropout |1\t|1\t|1\t|1\t|0.8|\t1|\t1|\t0.4|\t0.4\n",
    "Activation functions  |Log+Tanh|\tLog+Tanh|Log+Tanh\t|Log+Tanh|\tLog+Tanh\t|Log+Tanh|\tLog+Tanh\t|reLU+SM|\treLU+SM\t\n",
    "Batch normalization     |n\t|n\t|n\t|n\t|n|\tn|\tn|\tn|\tn\n",
    " Weight Decay    | 0\t|0\t|0\t|0.05\t|0.05|\t0.1|\t0.2|\t0.1|\t0.1\n",
    "Learning Rate     | 0.01\t|0.005\t|0.005|\t0.005\t|0.005\t|0.005|\t0.005\t|0.5\t|0.5\n",
    "Early Stopping      |  Y\t|Y\t|Y\t|Y\t|Y\t|Y\t|Y\t|Y\t|N\t\n",
    "Epochs   |  500|\t500|\t500|\t500\t|500\t|500\t|500\t|500\t|500\n",
    "Run time (s)    | 203.10\t|356.30\t|301.29|\t290.92|\t326.02\t|379.42\t|380.94\t|51.46|\t1636.56|\n",
    "Accuracy |88.8000%\t|88.8222%|\t88.7667%|\t88.8889%\t|87.0556%\t|88.9667%\t|88.9556%\t|10.0000%|\t10.0000%\n",
    "\\\\\n",
    "\n",
    "\n",
    "\n",
    "Experiment number  |19|\t20|\t21\t|22\t|23\t|24\t|25|\t26\n",
    ":--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :---\n",
    "Number hidden layers    |\t2|\t2  |   2\t|2\t|2\t|2\t|2|\t2\n",
    "Hidden layer node dimensions|69 : 99|69 : 99|69 : 99|69 : 99|69 : 99|69 : 99|69 : 99|69 : 99 \n",
    "Initialisation |\tnorm|\tnorm  |    norm\t|norm\t|norm\t|uni|\tuni|\tuni\n",
    "Dropout    |    0.4\t|0.4|\t1\t|0.2|\t0.2|\t0.2|\t0.4|\t0.4\n",
    "Activation functions |reLU+SM\t|reLU+SM | reLU+SM|\treLU+SM|\treLU+SM\t|reLU+SM\t|reLU+SM|\treLU+SM\n",
    "Batch normalization   |\tn|\tn  |    n\t|n\t|n\t|n\t|n\t|n\n",
    " Weight Decay  |\t0.1|\t0.1  |   0.05|\t0.05|\t0.05|\t0.1|\t0.1|\t0.1\n",
    "Learning Rate    |\t0.01\t|0.01 |  0.1\t|0.01\t|0.01\t|0.5\t|0.01\t|0.01\n",
    "Early Stopping   |Y\t|N   |   Y\t|Y\t|Y\t|Y\t|Y|\tN\n",
    "Epochs  \t|500\t|100 |   500\t|500|\t500\t|500|\t500\t|500\n",
    "Run time (s)  |\t52.08|\t439.40  | 51.97\t|55.91|\t81.83\t|54.43|\t51.51|\t1597.43\n",
    "Accuracy|\t10.0000%|\t10.0000% | 10.0000%\t|10.0000%\t|85.8333%|\t10.0000%\t|10.0000%\t|10.0000%\n",
    "\n",
    "\\\\\n",
    "\n",
    "\n",
    "\n",
    "Experiment number  \t26\t|27|\t28|\t29\t| 30 | 31\t|32\n",
    ":--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :---\n",
    "Number hidden layers    |3\t|3\t|3\t|3\t|3\t|3\t\t\n",
    "Hidden layer node dimensions|69 : 69 : 99|69 : 69 : 99|69 : 69 : 99|69 : 69 : 99|69 : 69 : 99|69 : 69 : 99\n",
    "Initialisation |norm\t|norm\t|norm\t|norm|\tnorm|\tnorm\n",
    "Dropout    |    1|1\t|1|\t1|\t1|\t1\n",
    "Activation functions |Log+Tanh\t|Log+Tanh\t|reLU+SM|\treLU+SM\t|reLU+SM|\treLU+SM\n",
    "Batch normalization   |n\t|n\t|n\t|n\t|n|\tn\n",
    " Weight Decay  |\t0.05\t|0|\t0.05|\t0.05|\t0.05|\t0.05\n",
    "Learning Rate    |\t0.01|\t0.01\t|0.01|\t0.01\t|0.01|\t0.01\n",
    "Early Stopping   |Y\t|Y|\tY|\tY|\tY|\tY\n",
    "Epochs  \t|500|500|500|10|11|12|\n",
    "Run time (s)  \t|212.0\t|180.7|\t156.6|\t48.7|\t54.7|\t59.5\n",
    "Accuracy|\t88.4667%|\t88.2889%|\t75.0444%|\t87.3000%|\t86.8222%|\t86.8889%\n",
    "\\\\\n",
    "\n",
    "\n",
    "Experiment number  |33 |34\t|\t35\t|\t36\t|\t37\t|\t38\t|\t39\n",
    ":--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :---|:---\n",
    "Number hidden layers |3|3|2|2|3|3|3\n",
    "Hidden layer node dimensions|69 : 69 : 99|69 : 69 : 99|69 : 99|69 : 99|99 : 69 : 39|99 : 69 : 39|99 : 69 : 39\n",
    "Initialisation |norm\t|uni|\tuni\t|uni\t|uni\t|uni |uni\n",
    "Dropout    |    1|\t1|0.9|\t0.9|\t0.9|\t0.9 | 0.8\n",
    "Activation functions |reLU+SM\t|Log+Tanh\t|Log+Tanh\t|LogSLogig+Tanh\t|Log+Tanh|\treLU+SM | leaky reLU+ SM\n",
    "Batch normalization   |n\t|n\t|n\t|n\t|n|\tn |n\n",
    " Weight Decay  |\t0.05\t|0|\t0.1|\t0.1|\t0.1|\t0.1 | 0\n",
    "Learning Rate    |\t0.01\t|0.01\t|0.005\t|0.005|\t0.005|\t0.005 | 0.001\n",
    "Early Stopping   | Y\t|Y|\tY|\tY|\tY|\tY | Y\n",
    "Epochs  \t|20|500|\t500\t|500|\t500\t|500 | 500\n",
    "Run time (s)  \t|99.1|\t196.7\t|524.0|\t454.0\t|532.0|\t250.0 | 441.2\n",
    "Accuracy|\t85.6667%\t|87.9778%\t|88.4800%\t|88.5100%\t|88.5400%\t|88.8700% | 89.600%\n",
    "\\\\\n",
    "\n",
    "![accuracy figure](https://drive.google.com/uc?authuser=0&id=1yNt-vFfc5VDEdrDkTdLKcCpSNhGiUGRa&export=download)\n",
    "*Figure 1: Accuracy and runtime across all experiments*\n",
    "\n",
    "\n",
    "Above is a graph of the accuracy across all our experiments, with the optimal model at the rightmost column. It can be seen that this has near highest accuracy and a middle of the range runtime. This is discussed further in section 4.0.\n",
    "\n",
    "## 3.2 Learning Rate\n",
    "Learning rate (LR) is a hyper-parameter that dictates how large the adjustment of weights and biases are with respect to the calculated gradient. It controls how quickly quickly or slowly a network is able to learn a problem. Reducing the learning rate reduces the size of the steps towards the local minima (i.e. the best accuracy). A small learning rate helps ensure that the local minima is not missed but it also slows down convergence. A larger learning rate speeds up convergence with larger steps but may miss the local minima. Four different values of LR were used in the random grid search. A summary of results is shown below. It can be seen that as the LR decreases mean accuracy and run time increase.\n",
    "\n",
    "Learning Rate | Metric | Mean | SD | Count\n",
    ":---:|:---:|:---:|:---:|:---:\n",
    "0.005|Acc|88.576%|0.749%|6\n",
    "0.005|Runtime (sec)|339.1|\t39.0|\t6\n",
    "0.001|Acc|86.283%|3.650%|13\n",
    "0.001\t|Runtime (sec)|\t178.4|\t209.6|\t13\n",
    "0.1|Acc|NA|NA|3\n",
    "0.5|Acc|NA|NA|5\n",
    "\n",
    "All networks with LR greater than 0.01 failed to converge. This is most likely the result of overflow (i.e. exploding gradients). Overflow occurs when network updates $(LR*gradient)$ become extremely large due to accumulation through the network. Exponential growth occurs by repeatedly multiplying gradients through the network layers that have values greater than one. Deeper networks and larger learning rates increase the likelihood of overflow. Appropriate selection of weight initialisation can reduce overflow. For neural networks using the reLU activation function 'He' initialization was employed. For neural networks using the logistic activation function the 'Xavier' initialization was used.\n",
    "\n",
    "## 3.3 Activation Functions\n",
    "Two activation functions were used in the random grid trails:\n",
    "1.   Logistic hidden layers with tanh output layer.\n",
    "2.   reLU hidden layer activation with softmax output activation. \n",
    "\n",
    "On average the logistic - tanh combination outperformed reLU - softmax on accuracy but not on run time. The quicker run time of reLU - softmax was an expected result. reLU is a simpler function with a simple derivative making backward and forward propagation faster. Dispite logistic - tanh activation function performing better on average, peak accuracy was higher using the reLU activation function. The peak accuracy achieved by reLU versus logistic was marginal (88.87% vs 88.48% respectively) but the difference in runtime was significant (250s vs 524s respectively).\n",
    "\n",
    "Activation Function | Metric | Mean | SD | Count\n",
    ":---:|:---:|:---:|:---:|:---:\n",
    "Log+Tanh\t|Acc (%)\t|88.497%\t|0.497%|\t13\n",
    "Log+Tanh\t|Runtime (sec)|\t282.6\t|72.8|\t13\n",
    "reLU+SM\t|Acc (%)\t|85.637%\t|4.0.81%|9\n",
    "reLU+SM|\tRuntime (sec)\t|168.5|\t259.3|\t9\n",
    "\n",
    "\n",
    "Softmax was used exclusively for the last layer of the neural networks because it provides an output resembling a probability distribution. Tanh was used prodominately as an output but can be used in hidden layers because it is a sigmoid function. Tanh differs from logistic in that it maps input bewteen -1 and 1 (logistic maps inputs between 0 and 1). An additional trial was conducted to compare a neural network with all tanh activation functions versus one with all logistic activation functions. Accuracy results were compariable however the tanh network had a significantly faster run time. This is because the tanh function produces stronger gradients resulting in faster convergence *(LeCun et al, 2012)*.\n",
    "\n",
    "\n",
    "Learning Rate | Acc (%) | Runtime (sec) \n",
    ":---:|:---:|:---:\n",
    "Logistic|88.6%|298.2\n",
    "Tanh|88.67%|167.6\n",
    "Log+tanh|88.8%|324.4\n",
    "\n",
    "## 3.4 Weight Initialization\n",
    "Two methods for weight initialization were investigated – these are outlined in section 2. The methods (He and Xavier) are used for setting the bounds of distributions from which weights are drawn. The method is selected based on the activation functions of a given layer, so these are not considered hyperparameters of the model. What is made available to the user however is the distribution from which weights are drawn.\n",
    "\n",
    "Our random grid search explored selection of weights from both a uniform and a truncated normal distribution. The results are as follow:\n",
    "\n",
    "\n",
    "Distribution |  Acc (%) mean\t| stddev\t\n",
    ":---:|:---:|:---:\n",
    "Normal |\t87.14%\t| 1.09%\n",
    "Uniform |\t88.58% |\t0.75%\n",
    "\n",
    "Using a uniform distribution has a better accuracy, with a tighter standard deviation. The original Xavier paper *(Glorot et al, 2010)* does use a uniform distribution – so perhaps this is not a surprise. The He paper *(He et al, 2015)* prefers a normal distribution, which is interesting when one considers the following breakout:\n",
    "\n",
    "\n",
    "Algorithm | Distribution |  Acc (%) mean\t| stddev\t\n",
    ":---:|:---:|:---:|:---:\n",
    "Xavier | Normal |\t88.43% |\t0%\n",
    "Xavier | Uniform |\t88.58%\t| 0.75%\n",
    "He | Normal |\t87.14%\t| 1.09%\n",
    "He | Uniform |\tNA | \tNA \n",
    "\n",
    "Uniform Xavier does perform better than normal-distribution Xavier. What is more interesting is that uniform-based He did not converge during our experiments. Whether this is due to confounding factors (ie other modules selected in the random grid) could be the topic of further investigation.\n",
    "\n",
    "## 3.5 Weight Decay\n",
    "Weight decay is a regularization method which penalizes large weights with a gradient proportional to the size of the weight. 2 values were made available in our random grid search. The results follow:\n",
    "\n",
    "\n",
    "Weight Decay | Acc (%)\tmean |\tstddev\n",
    ":---:|:---:|:---:|\n",
    "0 |\t88.80% |\t0.03%\n",
    "0.05 |\t87.42%\t| 1.11%\n",
    "\n",
    "Weight decay decreases accuracy. This may be due to our using it in parallel with 2 other forms of regularization – early stopping and dropout. Early stopping does not penalize large weights in particular, but attempts to stop training before overfitting occurs. Dropout removes weights at random attempting to strengthen lesser used paths. Both of these may be providing enough regularization to prevent overfitting meaning the indiscriminate regularization of weight decay only negatively affects the result.\n",
    "\n",
    "## 3.6 Dropout\n",
    "A range of potential probabilities from 0 to 1 for dropout were explored. There is a predicted zone the dropout probability is said to produce the best results. This is explored in part 7.3 of this paper *(Srivastava et al. 2014)*\n",
    "The whole range was however explored to understand if the data-set being modeled is representative in this respect. \n",
    "\n",
    "\n",
    "p Dropout | Accuracy | Run time (s)\n",
    ":---:|:---:|:---:\n",
    "0.2|83.3%|88\n",
    "0.3|85.6%|173\n",
    "0.4|87.4%|110\n",
    "0.5|87.9%|115\n",
    "0.6|87.6%|138\n",
    "0.7|88.2%|82\n",
    "0.8|88.4%|86\n",
    "0.9|88.4%|58\n",
    "1|87%|67\n",
    "\n",
    "From the results in the table above the effective range for the probability of dropout is slightly larger than the optimal zone stated in the paper. While there is only a negligible difference in accuracy between 0.9 and 0.8,  0.8 should regularise the model more and therefor chosen in our final model.\n",
    "\n",
    "## 3.7 Batch Normalisation\n",
    "Batch Normalization can affect the derivative characteristics of the Gradient Decent performed on the model. This typically speeds up convergence for a model. This was investigating by training a model with the same hyper-parameters with and without Batch Normalization. \n",
    "\n",
    "Uses Dropout | Accuracy | Run time (s)\n",
    ":---:|:---:|:---:\n",
    "True|87.6%|202\n",
    "False|88.9%|599\n",
    "\n",
    "As expected the model with Batch Normalization converged much faster than the model without. However accuracy did suffer slightly so it was not included as part of the optimal model.\n",
    "\n",
    "## 3.8 Number of Layers\n",
    "The universal approximation theorem states that an artificial neural network with one hidden layer can approximate any function. There are papers that found this to be true *(Ba et al. 2014)*. With this in mind it isn't immediately obvious if adding more layers to a model will aid in improving accuracy.\n",
    "\n",
    "Number of Hidden Layers | Accuracy | Run time (s)\n",
    ":---:|:---:|:---:\n",
    "1|89.1%|367\n",
    "2|89.3%|307\n",
    "3|89.6%|443\n",
    "4|89.3%|416\n",
    "5|89.4%|484\n",
    "6|89.3%|381\n",
    "\n",
    "While keeping all other hyper-parameters fixed we can see that model depth doesn't improve accuracy. This is in line which what was stated in the paper, which is that shallow neural networks can be trained to achieve results similar to well trained deeper networks.\n",
    "\n",
    "## 4.0 Optimal Model\n",
    "\n",
    "The random grid search experiment 15 had the highest accuracy. This was used as a base for further investigation - with manual tweaking of hyperparameters with the interest of increasing generalization as well as accuracy. The final, optimal model had the following features:\n",
    "\n",
    "\n",
    "Parameter | Value\n",
    ":---:|:---:|\n",
    "Number hidden layers | 3\n",
    "Hidden layer node dimensions|99 : 69 : 39\n",
    "Weight Initialisation |uniform\n",
    "Dropout    |  0.8\n",
    "Activation functions |\tleakyreLU+SM\n",
    "Batch normalization   | n\n",
    " Weight Decay  |\t0\n",
    "Learning Rate    |\t0.001\n",
    "Early Stopping   | Y\n",
    "Max Epochs  \t| 500\n",
    "Run time (s)  \t| 441.2\n",
    "Accuracy|\t89.600%\n",
    "\\\n",
    "\n",
    "The depth of the network was increased to 3 as this led to a small bump in accuracy (0.5% gain). The decision was also made to add 20% dropout  to the model to aid generalization, as it was felt that real world data may not exhibit the exact same characteristics as the training data.\n",
    "\n",
    "Weight decay was set to zero as it had not helped when used in unison with other regularization methods. The largest change was the addition of the leaky reLU activation function. This was implemented after many of the reLU experiments in the random grid search suffered from exploding gradients and dead reLUs.\n",
    "\n",
    "The final model had an accuracy of 89.6% which was the largest seen during our experimentation.\n",
    "\n",
    "### 4.1 Instructions for running\n",
    "\n",
    "The optimal model can be trained by calling train_optimal(data, labels). The final cell of this notebook contains code for training and predicting on the test set.\n",
    "\n",
    "## 5.0 Discussion\n",
    "\n",
    "The final model achieved accuracy of 89.6%. Despite the inclusion of extra regularization (dropout at 20%), this was the highest accuracy achieved in all experiments. The layout was selected based on the results of the random grid search, however the activation function of leaky-reLU was added at this later stage - implemented after many of the reLU experiments in the random grid search suffered from exploding gradients and dead reLUs. \n",
    "\n",
    "The random grid search is an effective method of quickly trialling combinations of modules. Given the highly dependent nature of the effect of different modules in a neural network, exploring hyperparameters separately gives no indication what will happen when they are employed together. Due to the large number of hyperparameters of our model, a traditional grid search would be unfeasible. The random grid search sees many of the benefits of a traditional grid search, inasfar as it explores interplay between hyperparameters, in far less time. It was unfortunate however that due to time constraints we couldn’t include all modules in our random grid search.\n",
    "\n",
    "\n",
    "Minibatch was used for all experiments due to time constraints, with other fitting methods (ie momentum and batch normalization) only explored in early experimental stages and as a test on our final model. Early stopping was used for similar reasons.\n",
    "\n",
    "\n",
    "Other combinations of activation functions (ie Sigmoid+Sigmoid) could have been investigated in the grid, however early experimentation pointed to Sigmoid+Tanh being the best combination of logistic functions. The same experiment was run on our final model (Sigmoid + TanH vs reLU vs all-sigmoid vs all-tanh). This was where leaky-reLU was selected as finally the best activation method with this combination of hyper-parameters.\n",
    "\n",
    "Another method we could have investigated given more time was much larger network layers (ie n=1000) with some enforcement of sparsity. \n",
    "\n",
    "We had much difficulty with the implementation of batch normalization – to learn the two variables gamma and beta requires an extension of the backpropagation algorithm, which required understanding the existing code and extending it. This feature was only completed in the final days of the project, meaning that much of the experimentation was already complete. It was, however, a very valuable endeavour to come to understand the delta-rule-based backpropagation method employed by the template code. It is impressive how highly parallel and simple this method is compared to separately calculating partial derivatives for each node in a layer with respect to each output node.\n",
    "\n",
    "It is assumed by inspecting the input data that this is a modified version of an image classification problem – potentially fashion-MNIST. The data has likely been run through a convolution and pooling step before being given to us. If this is the case, our final accuracy of around 89% is quite acceptable, given that human performance on this task is around the 83% mark *(Xiao et al, 2017)*. Human-like performance is often the goal of such tasks – achieving much better than human performance can be near impossible given potential noise in the labels which were generated by humans themselves.\n",
    "\n",
    "## 6.0 Conclusion\n",
    "\n",
    "Tuning of deep learning systems is a difficult and time consuming process. The interplay between the effects of different hyper-parameters on model performance, along with the exploding gradients problem and other non-convergence effects, means that a random grid search is one of the better methods of tuning.\n",
    "\n",
    "With the assumption that this is an MNIST-like problem, we have achieved accuracy similar to the state of the art of around 90%. Given infinite resources, a full grid search may have found an extra boost in accuracy but the decision was made to favour generalization over optimum accuracy. This is due to the expectation that real-world data may exhibit characteristics not seen in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGkj2RPTaJ9g"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import h5py\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DOSt_vG94Mfi"
   },
   "outputs": [],
   "source": [
    "# need to normalize input data to avoid overflow/underflow in initial epochs\n",
    "# normalize each feature independently\n",
    "# options are zscore, minmax\n",
    "def preprocess(input_array, method='zscore'):\n",
    "  if method == 'zscore':\n",
    "    for i in range(input_array.shape[1]):\n",
    "      mean = np.mean(input_array[:, i])\n",
    "      std = np.std(input_array[:, i])\n",
    "      input_array[:, i] = (input_array[:, i] - mean) / std\n",
    "  elif method == 'minmax':\n",
    "    for i in range(input_array.shape[1]):\n",
    "      # range 0 to max\n",
    "      input_array[:, i] = (input_array[:, i] - np.min(input_array[:, i]))\n",
    "      # range 0 to 2\n",
    "      input_array[:, i] /= (np.max(input_array[:, i]) / 2)\n",
    "      # range -1 to 1\n",
    "      input_array[:, i] -= 1\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hCFYVtU06MPR"
   },
   "outputs": [],
   "source": [
    "#use stratified sampling to split train into train/validation\n",
    "#this dataset is actually balanced but still good practice\n",
    "def split(dataset, labels, train_percent=.85):\n",
    "  count = len(dataset)\n",
    "  num_classes = np.max(label) + 1\n",
    "  train = []\n",
    "  train_target = []\n",
    "  validate = []\n",
    "  validate_target = []\n",
    "  for i in range(num_classes):\n",
    "    class_data = np.ravel(np.argwhere(label == i))\n",
    "    np.random.shuffle(class_data)\n",
    "    cutoff = int(len(class_data) * train_percent)\n",
    "    train_idx = class_data[:cutoff]\n",
    "    val_idx = class_data[cutoff:]\n",
    "    train.append(dataset[train_idx])\n",
    "    train_target.append(labels[train_idx])\n",
    "    validate.append(dataset[val_idx])\n",
    "    validate_target.append(labels[val_idx])\n",
    "    \n",
    "  return np.vstack(train), np.hstack(train_target), np.vstack(validate), np.hstack(validate_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsZmOeyySQq9"
   },
   "outputs": [],
   "source": [
    "#need to one-hot encode labels to map to N output nodes (1 per class)\n",
    "#ie convert each label into a (10,) vector where the relevant column is 1\n",
    "\n",
    "def OHE(input_array, num_classes=10):\n",
    "  output = []\n",
    "  for x in input_array:\n",
    "    output.append(np.zeros((10,)))\n",
    "    output[-1][x] = 1\n",
    "  return np.vstack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8KpB-EXchIh"
   },
   "outputs": [],
   "source": [
    "## implemented formulae from here: https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404\n",
    "class InitWeights(object):\n",
    "  def xavier(self, n_in, n_out, uniform=True):\n",
    "    if uniform:\n",
    "      bounds = np.sqrt(6) / (np.sqrt(n_in + n_out))\n",
    "      return self._uniform(n_in, n_out, bounds) \n",
    "    else:\n",
    "      stddev = np.sqrt(2) / (np.sqrt(n_in + n_out))\n",
    "      return self._truncated_normal(n_in, n_out, stddev)\n",
    "    \n",
    "  def he(self, n_in, n_out, uniform=True):\n",
    "    if uniform:\n",
    "      bounds = np.sqrt(2) / (np.sqrt(n_in))\n",
    "      return self._uniform(n_in, n_out, bounds)      \n",
    "    else:\n",
    "      stddev = np.sqrt(6) / (np.sqrt(n_in))\n",
    "      return self._truncated_normal(n_in, n_out, stddev)\n",
    " \n",
    "  def _uniform(self, n_in, n_out, bounds):\n",
    "    W = np.random.uniform(\n",
    "        low=-bounds,\n",
    "        high=bounds,\n",
    "        size=(n_in, n_out)\n",
    "      )\n",
    "    return W\n",
    "  \n",
    "  def _truncated_normal(self, n_in, n_out, stddev):\n",
    "    W = np.random.normal(\n",
    "        loc=0,\n",
    "        scale=stddev,\n",
    "        size=(n_in, n_out)\n",
    "      )\n",
    "    #truncate results - anything > 2 stddev out gets clipped\n",
    "    W[W> 2*stddev] = 2*stddev\n",
    "    W[W<-2*stddev] = -2*stddev\n",
    "    return W\n",
    "  \n",
    "  def __init__(self, init_method=\"xavier\"):\n",
    "    if init_method==\"xavier\":\n",
    "      self.f = self.xavier\n",
    "    elif init_method==\"he\":\n",
    "      self.f = self.he"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "coc2QCMsn63E"
   },
   "outputs": [],
   "source": [
    "def calc_MSE(y, y_hat):\n",
    "  error = y-y_hat\n",
    "  return np.mean(np.sum(error**2, axis=1))\n",
    "\n",
    "def labels_from_preds(preds):\n",
    "  return np.argmax(preds, axis=1)\n",
    "\n",
    "def calc_accuracy(labels, target):\n",
    "  return np.sum(labels == target) / len(target)\n",
    "\n",
    "#wasn't sure if we could use a package to shuffle so found this code: https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kYXDLyEWGG2r"
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "class Activation(object):\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def logistic(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  a * (1 - a )\n",
    "      \n",
    "    def ReLU(self, x):\n",
    "        x[x<0] =0\n",
    "        return x\n",
    "      \n",
    "    def ReLU_deriv(self, a):\n",
    "        der = np.zeros(a.shape)\n",
    "        der[a>0] =1\n",
    "        return der\n",
    "      \n",
    "    def leaky_ReLU(self, x):\n",
    "        x = np.where(x > 0, x, x*0.01)\n",
    "        return x\n",
    "      \n",
    "    def leaky_ReLU_deriv(self, a):\n",
    "        der = np.full(a.shape, 0.01)\n",
    "        der[a>0] =1\n",
    "        return der\n",
    "      \n",
    "    def softmax(self, x):\n",
    "        # apply max normalization to avoid overflow\n",
    "        if len(x.shape) > 1:\n",
    "          x_norm = (x.T - np.max(x, axis=1)).T\n",
    "          return softmax(x_norm, axis=1)\n",
    "        else:\n",
    "          x_norm = x - np.max(x)\n",
    "          return softmax(x_norm)\n",
    "      \n",
    "    def softmax_deriv(self, a):\n",
    "        return np.ones(a.shape)\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.logistic\n",
    "            self.f_deriv = self.logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.tanh\n",
    "            self.f_deriv = self.tanh_deriv\n",
    "        elif activation == 'relu':\n",
    "            self.f = self.ReLU\n",
    "            self.f_deriv = self.ReLU_deriv\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.f = self.leaky_ReLU\n",
    "            self.f_deriv = self.leaky_ReLU_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.softmax\n",
    "            self.f_deriv = self.softmax_deriv\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HbjWqZ24L3Ot"
   },
   "outputs": [],
   "source": [
    "class Loss_function(object):\n",
    "    def MSE(self, y, y_hat):\n",
    "        error = y-y_hat\n",
    "        loss=np.sum(error**2)\n",
    "        return loss\n",
    "      \n",
    "    def Cross_entropy(self, y, y_hat):\n",
    "        return -np.log(y_hat[np.argmax(y)])\n",
    "      \n",
    "    def l2_reg(self, reg_weight, layers, sample_weight):\n",
    "        accum = 0\n",
    "        for layer in layers:\n",
    "          accum += np.sum(np.square(layer.W))\n",
    "          \n",
    "        return accum*reg_weight*sample_weight/2\n",
    "        \n",
    "    def __init__(self,loss='cross_entropy'):\n",
    "        if loss == 'MSE':\n",
    "            self.loss = self.MSE\n",
    "        elif loss == 'cross_entropy':\n",
    "            self.loss = self.Cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BeoakGoCGLvb"
   },
   "outputs": [],
   "source": [
    "\n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer='tanh',activation='tanh', W=None, b=None,\n",
    "                init_uniform=True, weight_decay=None, last_layer=False):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        :type init_uniform: bool\n",
    "        :param init_uniform: Whether to draw init weights from uniform dist (else normal)\n",
    "        \n",
    "        :type weight_decay: float/None/False\n",
    "        :param weight_decay: Weight to apply to l2 reg loss factor (else none/false)\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.dropout_cache=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.last_layer=last_layer\n",
    "        \n",
    "        if activation=='relu':\n",
    "          self.init_weights = InitWeights(\"he\").f\n",
    "        else:\n",
    "          self.init_weights = InitWeights(\"xavier\").f\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        if W is not None:\n",
    "          self.W = W\n",
    "        else:\n",
    "          self.W = self.init_weights(n_in, n_out, init_uniform)\n",
    "\n",
    "        if b is not None:\n",
    "          self.b = b\n",
    "        else:\n",
    "          self.b = np.zeros(n_out,)  \n",
    "          \n",
    "        self.weight_decay = weight_decay\n",
    "          \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "        # create arrays to store the velocity values for momentum calculation\n",
    "        self.vW = np.zeros(self.W.shape)\n",
    "        self.vb = np.zeros(self.b.shape)\n",
    "        \n",
    "        \n",
    "        # Create arrays to store the gamma and beta values for batch norm. @\n",
    "        \n",
    "        self.bn_mean = np.zeros((1,n_out))\n",
    "        self.bn_var = np.zeros((1,n_out))\n",
    "        \n",
    "        self.bn_gamma = np.ones((1,n_out))\n",
    "        self.bn_beta = np.ones((1,n_out))\n",
    "        \n",
    "        self.grad_bn_gamma = np.zeros((1,n_out))\n",
    "        self.grad_bn_beta = np.zeros((1,n_out))\n",
    "        \n",
    "        self.layer_mu = np.zeros((1,n_out))\n",
    "        self.layer_var = np.zeros((1,n_out))\n",
    "        \n",
    "        self.lin_output = np.zeros((1,n_out))\n",
    "        self.lin_output_norm = np.zeros((1,n_out))\n",
    "    \n",
    "    def dropout(self, nodes, probability):\n",
    "        #This distribution decides what nodes will be on or not. We then rescale the on nodes proportionally to the probability that it is off.\n",
    "        \n",
    "        active_nodes = np.random.binomial(1, probability, size=nodes.shape) / probability\n",
    "        output = np.multiply(nodes, active_nodes)\n",
    "        return output, active_nodes\n",
    "    \n",
    "    def forward(self, input, probability):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input=input\n",
    "        \n",
    "        if self.last_layer:\n",
    "            probability=1\n",
    "        \n",
    "        self.output, self.dropout_cache = self.dropout(self.output, probability)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta, output_layer=False, sampleweight=1):\n",
    "        delta *= self.dropout_cache\n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
    "        \n",
    "        if self.weight_decay:\n",
    "            self.grad_W += self.W * self.weight_decay * sampleweight\n",
    "        self.grad_b = np.sum(delta, axis=0)\n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta\n",
    "      \n",
    "    def forward_BN(self, input, probability, fit):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        self.lin_output = np.dot(input, self.W) + self.b\n",
    "        \n",
    "        '''\n",
    "        This is where batch norm is implemented\n",
    "        '''\n",
    "        if self.last_layer:\n",
    "            probability=1\n",
    "        else:\n",
    "            if fit:\n",
    "\n",
    "              self.layer_mu = np.mean(self.lin_output, axis=0)\n",
    "              self.layer_var = np.var(self.lin_output, axis=0)\n",
    "\n",
    "              self.lin_output_norm = (self.lin_output - self.layer_mu) / np.sqrt(self.layer_var + 1e-8)\n",
    "\n",
    "\n",
    "              self.bn_mean = 0.9 * self.bn_mean + (1 - 0.9 ) *  self.layer_mu\n",
    "              self.bn_var = 0.9 * self.bn_var + (1 - 0.9 ) *  self.layer_var\n",
    "            else:\n",
    "              self.lin_output_norm = (self.lin_output - self.bn_mean) / np.sqrt(self.bn_var + 1e-8)\n",
    "\n",
    "            self.lin_output = self.bn_gamma * self.lin_output_norm + self.bn_beta\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.output = (\n",
    "            self.lin_output if self.activation is None\n",
    "            else self.activation(self.lin_output)\n",
    "        )\n",
    "        self.input=input\n",
    "        \n",
    "\n",
    "        \n",
    "        self.output, self.dropout_cache = self.dropout(self.output, probability)\n",
    "        return self.output                                       \n",
    "\n",
    "    def backward_BN(self, delta, output_layer=False, sampleweight=1):\n",
    "        \n",
    "\n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
    "        \n",
    "        if self.weight_decay:\n",
    "            self.grad_W += self.W * self.weight_decay * sampleweight\n",
    "        self.grad_b = np.sum(delta, axis=0)\n",
    "\n",
    "            \n",
    "        delta *= self.dropout_cache\n",
    "           \n",
    "        if not self.last_layer:  \n",
    "            N, D = self.lin_output.shape\n",
    "\n",
    "\n",
    "            input_mu = (self.lin_output - self.layer_mu)\n",
    "            std_inv = 1. / np.sqrt(self.layer_var + 1e-8)\n",
    "\n",
    "            dinput_norm = delta * self.bn_gamma\n",
    "            dvar = np.sum(dinput_norm * input_mu, axis=0) * -0.5 * (self.layer_var + 1e-8)**(-3/2)\n",
    "            dmu = np.sum(dinput_norm * -std_inv, axis=0) + dvar * np.mean(-2. * input_mu, axis=0)\n",
    "\n",
    "            delta = (dinput_norm * std_inv) + (dvar * 2 * input_mu / N) + (dmu / N)\n",
    "            self.grad_bn_gamma = np.sum(delta * self.lin_output_norm, axis=0)\n",
    "            self.grad_bn_beta = np.sum(delta, axis=0)\n",
    "        \n",
    "        \n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "                \n",
    "        return delta\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LKgiGhdGQbS"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\"      \n",
    "    def __init__(self, layers=None, activation=[None,'tanh','tanh'], init_uniform=True, weight_decay=False, from_file=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        :param init_uniform: Whether to draw init weights from uniform dist (else normal)\n",
    "        :param weight_decay: lambda for strength of l2 regularization on weights (else False/None for no reg)\n",
    "        :param from_file: a file to load to get pretrained weights. \n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params= {'activation':activation, 'layers':layers, 'weight_decay': weight_decay, 'init_uniform': init_uniform}\n",
    "        \n",
    "        self.es_epochs=None\n",
    "        if from_file:\n",
    "          dumped_model = self._load_model(from_file)\n",
    "          self.params = dumped_model['params']\n",
    "          self.activation=self.params['activation']\n",
    "          layers = self.params['layers']\n",
    "          init_uniform = self.params['init_uniform']\n",
    "          for i in range(len(self.params['layers'])-1):\n",
    "              if i==len(self.params['layers'])-2:\n",
    "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],  \n",
    "                                              W=dumped_model['weights'][i][0], b=dumped_model['weights'][i][1], weight_decay=weight_decay, init_uniform=init_uniform,last_layer=True))\n",
    "              else:\n",
    "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],  \n",
    "                                              W=dumped_model['weights'][i][0], b=dumped_model['weights'][i][1], weight_decay=weight_decay, init_uniform=init_uniform))\n",
    "        else:\n",
    "          self.activation=activation\n",
    "          for i in range(len(layers)-1):\n",
    "              if i==len(layers)-2:\n",
    "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1], weight_decay=weight_decay, init_uniform=init_uniform,last_layer=True))\n",
    "              else:\n",
    "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1], weight_decay=weight_decay, init_uniform=init_uniform))\n",
    "    \n",
    "    def forward(self,input, dropout_p=1):\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input, dropout_p)\n",
    "            input=output\n",
    "        return output\n",
    "    \n",
    "    #@ Can combine this with forward above with an if statement. Just wanted to keep separte for now until I know it works.\n",
    "    def Forward_BN(self,input, dropout_p=1, fit=True):\n",
    "        for layer in self.layers:\n",
    "            output =layer.forward_BN(input, dropout_p, fit=fit)\n",
    "            input=output\n",
    "        return output\n",
    "      \n",
    "    def set_early_stopping(self, validation, validation_labels, num_epochs=10):\n",
    "        # for early stopping\n",
    "        self.validation = validation\n",
    "        self.validation_labels = labels_from_preds(validation_labels)\n",
    "        self.es_epochs = num_epochs\n",
    "      \n",
    "    def calculate_loss(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        # call to loss function\n",
    "        loss=[]\n",
    "        delta=[]\n",
    "\n",
    "        for i, single_y in enumerate(y):\n",
    "          loss.append(Loss_function('MSE').loss(single_y, y_hat[i]))\n",
    "          error = single_y-y_hat[i]\n",
    "          # calculate the delta of the output layer\n",
    "          delta.append(np.array(-error*activation_deriv(y_hat[i])))\n",
    "        # return loss and delta\n",
    "        loss = np.array(loss)\n",
    "        if self.params['weight_decay']:\n",
    "          loss += Loss_function().l2_reg(self.params['weight_decay'], self.layers, len(y)/self.Xcount)\n",
    "\n",
    "        return loss,np.array(delta)\n",
    "        \n",
    "    def backward(self,delta, sampleweight):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True, sampleweight=sampleweight)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta, sampleweight=sampleweight)\n",
    "    \n",
    "    #@ can combine this with backwards above with an if statement. Just wanted to keep separte for now until I know it works.\n",
    "    def Backward_BN(self, delta, sampleweight):\n",
    "        delta =self.layers[-1].backward_BN(delta, output_layer=True, sampleweight=sampleweight)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta =layer.backward_BN(delta, sampleweight=sampleweight)                \n",
    "            \n",
    "    def update(self,lr):\n",
    "        for layer in self.layers:\n",
    "            layer.W -= lr * layer.grad_W\n",
    "            layer.b -= lr * layer.grad_b\n",
    "            \n",
    "    def update_momentum(self, lr, mom):\n",
    "        for layer in self.layers:\n",
    "            layer.vW = mom * layer.vW + lr * layer.grad_W\n",
    "            layer.vb = mom * layer.vb + lr * layer.grad_b\n",
    "            layer.W -= layer.vW\n",
    "            layer.b -= layer.vb \n",
    "            \n",
    "    def update_BN(self,lr):\n",
    "        for layer in self.layers:\n",
    "            layer.W -= lr * layer.grad_W\n",
    "            layer.b -= lr * layer.grad_b\n",
    "            layer.bn_gamma -= lr * layer.grad_bn_gamma\n",
    "            layer.bn_beta -= lr * layer.grad_bn_beta       \n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1, epochs=100, dropout_p=1):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\" \n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        self.Xcount = len(X)\n",
    "        if self.es_epochs:\n",
    "            validation_acc = np.zeros(epochs)\n",
    "        for k in range(epochs):\n",
    "            #print('epoch', k)\n",
    "            loss=np.zeros(X.shape[0])\n",
    "            for it in range(X.shape[0]):\n",
    "                i=np.random.randint(X.shape[0])\n",
    "                \n",
    "                # forward pass\n",
    "                y_hat = self.forward(X[i], dropout_p)\n",
    "                # backward pass\n",
    "                loss[it],delta=self.calculate_loss([y[i]],[y_hat])\n",
    "                self.backward(delta, 1/self.Xcount)\n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "            to_return[k] = np.mean(loss)\n",
    "            if not k % 10:\n",
    "              print(\".\", end=\"\")\n",
    "            if self.es_epochs:\n",
    "              preds = self.predict(self.validation)\n",
    "              validation_acc[k] = calc_accuracy(labels_from_preds(preds), self.validation_labels)\n",
    "              if k - np.argmax(validation_acc) > self.es_epochs:\n",
    "                print(\"Haven't improved accuracy on validation set in {} epochs, stopping\".format(self.es_epochs))\n",
    "                break\n",
    "        return to_return\n",
    "      \n",
    "    def fit_mb(self,X,y,mini_batch_size,learning_rate=0.1, epochs=100, dropout_p=1):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        :param early_stop: int: stop if haven't improved in this many epochs\n",
    "        \"\"\" \n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs) #array to store values of mean loss for each epoch for plotting later\n",
    "        if self.es_epochs:\n",
    "            validation_acc = np.zeros(epochs)\n",
    "        self.Xcount = len(X)\n",
    "        \n",
    "        for k in range(epochs): #for each epoch\n",
    "            X, y = shuffle_in_unison(X, y) #shuffle the input data and input targets\n",
    "            loss=np.zeros(X.shape[0]) #create array of zeros whose lengths = #samples.\n",
    "            \n",
    "            #partition training data (X, y) into mini-batches\n",
    "            for j in range(0, X.shape[0], mini_batch_size):\n",
    "              X_mini = X[j:j + mini_batch_size]\n",
    "              y_mini = y[j:j + mini_batch_size]\n",
    "              # forward pass\n",
    "              y_hat = self.forward(X_mini, dropout_p) #forward feed the mini_batches to get outputs (y_hat)\n",
    "              \n",
    "              # backwards pass\n",
    "              loss[j:j + mini_batch_size], delta=self.calculate_loss(y[j:j + mini_batch_size], y_hat) #input y and y_hat into calculate_loss. Output = loss and delta\n",
    "              self.backward(delta, mini_batch_size/self.Xcount) #pass delta from calculate_loss to backward.\n",
    "\n",
    "              # update\n",
    "              self.update(learning_rate)\n",
    "            to_return[k] = np.mean(loss) #add mean loss to to_return\n",
    "            if not k % 10:\n",
    "              print(\".\", end=\"\")\n",
    "            if self.es_epochs:\n",
    "              preds = self.predict(self.validation)\n",
    "              validation_acc[k] = calc_accuracy(labels_from_preds(preds), self.validation_labels)\n",
    "              if k - np.argmax(validation_acc) > self.es_epochs:\n",
    "                print(\"Haven't improved accuracy on validation set in {} epochs, stopping\".format(self.es_epochs))\n",
    "                break\n",
    "              \n",
    "        return to_return[:k]\n",
    "      \n",
    "    def fit_mb_BN(self,X,y,mini_batch_size,learning_rate=0.1, epochs=100, dropout_p=1):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        :param early_stop: int: stop if haven't improved in this many epochs\n",
    "        \"\"\" \n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs) #array to store values of mean loss for each epoch for plotting later\n",
    "        if self.es_epochs:\n",
    "            validation_acc = np.zeros(epochs)\n",
    "        self.Xcount = len(X)\n",
    "        \n",
    "        for k in range(epochs): #for each epoch\n",
    "            X, y = shuffle_in_unison(X, y) #shuffle the input data and input targets\n",
    "            loss=np.zeros(X.shape[0]) #create array of zeros whose lengths = #samples.\n",
    "            \n",
    "            #partition training data (X, y) into mini-batches\n",
    "            for j in range(0, X.shape[0], mini_batch_size):\n",
    "              X_mini = X[j:j + mini_batch_size]\n",
    "              y_mini = y[j:j + mini_batch_size]\n",
    "              # forward pass\n",
    "              y_hat = self.Forward_BN(X_mini, dropout_p, fit=True) #forward feed the mini_batches to get outputs (y_hat)\n",
    "              \n",
    "              # backwards pass\n",
    "              loss[j:j + mini_batch_size], delta=self.calculate_loss(y[j:j + mini_batch_size], y_hat) #input y and y_hat into calculate_loss. Output = loss and delta\n",
    "              self.Backward_BN(delta, mini_batch_size/self.Xcount) #pass delta from calculate_loss to backward.\n",
    "\n",
    "              # update\n",
    "              self.update_BN(learning_rate)\n",
    "            to_return[k] = np.mean(loss) #add mean loss to to_return\n",
    "            if not k % 10:\n",
    "              print(\".\", end=\"\")\n",
    "            if self.es_epochs:\n",
    "              preds = self.predict(self.validation)\n",
    "              validation_acc[k] = calc_accuracy(labels_from_preds(preds), self.validation_labels)\n",
    "              if k - np.argmax(validation_acc) > self.es_epochs:\n",
    "                print(\"Haven't improved accuracy on validation set in {} epochs, stopping\".format(self.es_epochs))\n",
    "                break\n",
    "              \n",
    "        return to_return[:k]\n",
    "      \n",
    "    def fit_SGD_momentum(self,X,y,learning_rate=0.1, epochs=100, momentum=0.9, dropout_p=1):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\" \n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        self.Xcount = len(X)\n",
    "        if self.es_epochs:\n",
    "            validation_acc = np.zeros(epochs)\n",
    "        for k in range(epochs):\n",
    "            loss=np.zeros(X.shape[0])\n",
    "            \n",
    "            # loop through training examples\n",
    "            for j in range(X.shape[0]):\n",
    "              i=np.random.randint(X.shape[0])\n",
    "                \n",
    "              # forward pass\n",
    "              y_hat = self.forward(X[i], dropout_p)\n",
    "                \n",
    "              # backward pass\n",
    "              loss[j],delta=self.calculate_loss([y[i]],[y_hat])\n",
    "              self.backward(delta, X.shape[0]/self.Xcount)\n",
    "                \n",
    "              # update\n",
    "              self.update_momentum(learning_rate, momentum)\n",
    "            to_return[k] = np.mean(loss)\n",
    "            if not k % 10:\n",
    "              print(\".\", end=\"\")\n",
    "            if self.es_epochs:\n",
    "              preds = self.predict(self.validation)\n",
    "              validation_acc[k] = calc_accuracy(labels_from_preds(preds), self.validation_labels)\n",
    "              if k - np.argmax(validation_acc) > self.es_epochs:\n",
    "                print(\"Haven't improved accuracy on validation set in {} epochs, stopping\".format(self.es_epochs))\n",
    "                break\n",
    "        return to_return  \n",
    "\n",
    "    def predict(self, x, bn=False):\n",
    "        x = np.array(x)\n",
    "        output = []\n",
    "        for i in np.arange(x.shape[0]):\n",
    "          if bn:\n",
    "            output.append(self.Forward_BN(x[i,:], fit=False))\n",
    "          else:\n",
    "            output.append(self.forward(x[i,:]))\n",
    "        return np.vstack(output)\n",
    "      \n",
    "\n",
    "    def save_model(self, name):\n",
    "      model = {'params':self.params, 'weights':[]}\n",
    "      for x in self.layers:\n",
    "        model['weights'].append((x.W, x.b))\n",
    "        \n",
    "      with open(model_dir.format(name), 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "        \n",
    "    def _load_model(self, name):\n",
    "      with open(model_dir.format(name), 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "2dTOURCJ0Uub",
    "outputId": "f241d3d8-c1a0-4374-817c-b633e9536e12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['uni' 'norm' 'norm' 'norm' 'norm' 'uni' 'norm' 'norm' 'uni' 'norm']\n",
      " ['0.4' '0.6' '0.6' '0.4' '0.6' '0.6' '0.6' '0.4' '0.8' '0.2']\n",
      " ['Sig+Tanh' 'Sig+Tanh' 'Sig+Tanh' 'reLU+SM' 'Sig+Tanh' 'reLU+SM'\n",
      "  'reLU+SM' 'reLU+SM' 'reLU+SM' 'Sig+Tanh']\n",
      " ['y' 'y' 'n' 'n' 'n' 'n' 'n' 'y' 'y' 'n']\n",
      " ['0.05' '0.1' '0.05' '0.1' '0.05' '0.1' '0.1' '0.1' '0.1' '0.1']\n",
      " ['0.1' '0.5' '0.01' '0.1' '0.01' '0.5' '0.5' '0.5' '0.5' '0.01']]\n"
     ]
    }
   ],
   "source": [
    "#Random Search Grid Generator\n",
    "import random\n",
    "\n",
    "no_samples = 10\n",
    "# Initilization (uniform, normal)\n",
    "init = ['uni', 'norm']\n",
    "init_sample = random.choices(init, k = no_samples)\n",
    "\n",
    "# Dropout (1, 0.8, )\n",
    "drop = [1, 0.8, 0.6, 0.4, 0.2]\n",
    "drop_sample = random.choices(drop, k = no_samples)\n",
    "\n",
    "# Activation function (Sigmoid+Tanh, reLU+softmax)\n",
    "af = ['Sig+Tanh', 'reLU+SM']\n",
    "af_sample = random.choices(af, k = no_samples)\n",
    "\n",
    "# Batch norm (Y/N)\n",
    "BN = ['y', 'n']\n",
    "BN_sample = random.choices(BN, k = no_samples)\n",
    "\n",
    "# Weight decay (0, 0.05, 0.1)\n",
    "weight_decay = [0, 0.05, 0.1]\n",
    "wd_sample = random.choices(weight_decay, k = no_samples)\n",
    "\n",
    "# Learning rate (0.01, 0.1, 0.5)\n",
    "LR = [0.01, 0.1, 0.5]\n",
    "LR_sample = random.choices(LR, k = no_samples)\n",
    "\n",
    "random_grid = np.vstack((init_sample, drop_sample, af_sample, BN_sample, wd_sample, LR_sample))\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DOPAt_9nGd5y"
   },
   "outputs": [],
   "source": [
    "def train_optimal(data, label, testing=False):\n",
    "  np.seterr(all=\"warn\")\n",
    "  np.random.seed(1)\n",
    "  procdata = np.copy(data)\n",
    "  preprocess(procdata, 'zscore')\n",
    "\n",
    "  #split data\n",
    "  train, train_target, validate, validate_target = split(procdata, label)\n",
    "  #one hot encode targets\n",
    "  train_target = OHE(train_target, 10)\n",
    "  validate_target = OHE(validate_target, 10)\n",
    "  target = OHE(label, 10)\n",
    "\n",
    "  \n",
    "  nn = MLP([128, 99, 70, 41, 10], [None,'leaky_relu','leaky_relu','leaky_relu', 'softmax'])\n",
    "  nn.set_early_stopping(validate, validate_target, 10)\n",
    "  start = time.time()\n",
    "  #run to find optimal stopping point with early stopping\n",
    "  MSE = nn.fit_mb(train, train_target, learning_rate=0.001,mini_batch_size=32, epochs=100, dropout_p=0.8)\n",
    "\n",
    "  nn = MLP([128, 99, 70, 41, 10], [None,'leaky_relu','leaky_relu','leaky_relu', 'softmax'])\n",
    "  if testing:\n",
    "    # testing\n",
    "    # refit on just training data but finish 10 epochs earlier\n",
    "    MSE = nn.fit_mb(train, train_target, learning_rate=0.001, mini_batch_size=32, epochs=len(MSE)-10, dropout_p=0.8)\n",
    "  else:\n",
    "    #production time\n",
    "    #run again on all data and finish 10 epochs earlier - optimum spot\n",
    "    MSE = nn.fit_mb(procdata, target, learning_rate=0.001, mini_batch_size=32, epochs=len(MSE)-10, dropout_p=0.8)\n",
    "\n",
    "  print(\"{}s to train\".format(time.time() - start))\n",
    "  print('loss:%f'%MSE[-1])\n",
    "  return MSE, nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "colab_type": "code",
    "id": "-U-Uf7i9YX9t",
    "outputId": "2d96e55b-353a-44f7-e300-599008d75c4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......Haven't improved accuracy on validation set in 10 epochs, stopping\n",
      ".....603.3681921958923s to train\n",
      "loss:0.135351\n",
      "Predicted the following distribution {} Counter({3: 6163, 4: 6123, 7: 6056, 8: 6018, 9: 5985, 6: 5984, 5: 5957, 1: 5944, 0: 5892, 2: 5878})\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "#TODO load data and label from local dir into data/label vars here and remove google drive code\n",
    "with h5py.File('../Input/train_128.h5','r') as H:\n",
    "  data = np.copy(H['data'])\n",
    "with h5py.File('../Input/train_label.h5','r') as H:\n",
    "  label = np.copy(H['label'])\n",
    "with h5py.File('../Input/test_128.h5','r') as H:\n",
    "  test_data = np.copy(H['data'])\n",
    "\n",
    "\n",
    "testing = False\n",
    "\n",
    "MSE, nn = train_optimal(data, label, testing)\n",
    "pl.figure(figsize=(15,4))\n",
    "pl.plot(MSE)\n",
    "pl.grid()\n",
    "\n",
    "procdata = np.copy(data)\n",
    "preprocess(procdata, 'zscore')\n",
    "\n",
    "if testing:\n",
    "  _, _, test_data, test_target = split(procdata, label)\n",
    "  test_target = OHE(test_target, 10)\n",
    "  preds = nn.predict(test_data)\n",
    "  acc = calc_accuracy(labels_from_preds(preds), labels_from_preds(test_target)) # we have to de-OHE the predictions and the target data\n",
    "  print(\"Final accuracy on test set was {}\".format(acc))\n",
    "else:\n",
    "  #load test data here\n",
    "  #preprocess the data\n",
    "  proctest = np.copy(data)\n",
    "  preprocess(proctest, 'zscore')\n",
    "\n",
    "  preds = nn.predict(proctest)\n",
    "  output = labels_from_preds(preds)\n",
    "  #save in h5 file in /Output\n",
    "  with h5py.File('../Output/Predicted_labels.h5','w') as H:\n",
    "    H['label'] = output\n",
    "    \n",
    "  c = Counter(output)\n",
    "  print(\"Predicted the following distribution {}\", c)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
