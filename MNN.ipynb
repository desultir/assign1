{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desultir/assign1/blob/master/MNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fQvPcbXfGAnt",
        "colab_type": "code",
        "outputId": "8d2af89d-8a24-434e-8cdb-5bd097f8331b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scipy\n",
        "!pip install --upgrade numpy\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scipy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/5f/c48860704092933bf1c4c1574a8de1ffd16bf4fde8bab190d747598844b2/scipy-1.2.1-cp36-cp36m-manylinux1_x86_64.whl (24.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 24.8MB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.14.6)\n",
            "\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mfastai 1.0.51 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "  Found existing installation: scipy 1.1.0\n",
            "    Uninstalling scipy-1.1.0:\n",
            "      Successfully uninstalled scipy-1.1.0\n",
            "Successfully installed scipy-1.2.1\n",
            "Collecting numpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 17.3MB 2.1MB/s \n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mdatascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.14.6\n",
            "    Uninstalling numpy-1.14.6:\n",
            "      Successfully uninstalled numpy-1.14.6\n",
            "Successfully installed numpy-1.16.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "GGkj2RPTaJ9g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as pl\n",
        "from ipywidgets import interact, widgets\n",
        "from matplotlib import animation\n",
        "import h5py\n",
        "from google.colab import drive\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kf3k_uTGGlqK",
        "colab_type": "code",
        "outputId": "16a220ca-3635-426c-9c7d-5cf618261756",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "with h5py.File('/content/drive/My Drive/Colab Notebooks/Input/train_128.h5','r') as H:\n",
        "  data = np.copy(H['data'])\n",
        "with h5py.File('/content/drive/My Drive/Colab Notebooks/Input/train_label.h5','r') as H:\n",
        "  label = np.copy(H['label'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rJ4v9obPcy13",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import pickle\n",
        "import glob\n",
        "\n",
        "model_dir = \"/content/drive/My Drive/Colab Notebooks/Models/{}.pk\"\n",
        "\n",
        "def list_models():\n",
        "  return glob.glob(model_dir.format(\"*\"))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOSt_vG94Mfi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# need to normalize input data to avoid overflow/underflow in initial epochs\n",
        "# normalize each feature independently\n",
        "# options are zscore, minmax\n",
        "def preprocess(input_array, method='zscore'):\n",
        "  if method == 'zscore':\n",
        "    for i in range(input_array.shape[1]):\n",
        "      mean = np.mean(input_array[:, i])\n",
        "      std = np.std(input_array[:, i])\n",
        "      input_array[:, i] = (input_array[:, i] - mean) / std\n",
        "  elif method == 'minmax':\n",
        "    for i in range(input_array.shape[1]):\n",
        "      # range 0 to max\n",
        "      input_array[:, i] = (input_array[:, i] - np.min(input_array[:, i]))\n",
        "      # range 0 to 2\n",
        "      input_array[:, i] /= (np.max(input_array[:, i]) / 2)\n",
        "      # range -1 to 1\n",
        "      input_array[:, i] -= 1\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hCFYVtU06MPR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#use stratified sampling to split train into train/validation\n",
        "#this dataset is actually balanced but still good practice\n",
        "def split(dataset, labels, train_percent=.85):\n",
        "  count = len(dataset)\n",
        "  num_classes = np.max(label) + 1\n",
        "  train = []\n",
        "  train_target = []\n",
        "  validate = []\n",
        "  validate_target = []\n",
        "  for i in range(num_classes):\n",
        "    class_data = np.ravel(np.argwhere(label == i))\n",
        "    np.random.shuffle(class_data)\n",
        "    cutoff = int(len(class_data) * train_percent)\n",
        "    train_idx = class_data[:cutoff]\n",
        "    val_idx = class_data[cutoff:]\n",
        "    train.append(dataset[train_idx])\n",
        "    train_target.append(labels[train_idx])\n",
        "    validate.append(dataset[val_idx])\n",
        "    validate_target.append(labels[val_idx])\n",
        "    \n",
        "  return np.vstack(train), np.hstack(train_target), np.vstack(validate), np.hstack(validate_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RsZmOeyySQq9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#need to one-hot encode labels to map to N output nodes (1 per class)\n",
        "#ie convert each label into a (10,) vector where the relevant column is 1\n",
        "\n",
        "def OHE(input_array, num_classes=10):\n",
        "  output = []\n",
        "  for x in input_array:\n",
        "    output.append(np.zeros((10,)))\n",
        "    output[-1][x] = 1\n",
        "  return np.vstack(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q8KpB-EXchIh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## implemented formulae from here: https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404\n",
        "class InitWeights(object):\n",
        "  def xavier(self, n_in, n_out, uniform=True):\n",
        "    if uniform:\n",
        "      bounds = np.sqrt(6) / (np.sqrt(n_in + n_out))\n",
        "      return self._uniform(n_in, n_out, bounds) \n",
        "    else:\n",
        "      stddev = np.sqrt(2) / (np.sqrt(n_in + n_out))\n",
        "      return self._truncated_normal(n_in, n_out, stddev)\n",
        "    \n",
        "  def he(self, n_in, n_out, uniform=True):\n",
        "    if uniform:\n",
        "      bounds = np.sqrt(2) / (np.sqrt(n_in))\n",
        "      return self._uniform(n_in, n_out, bounds)      \n",
        "    else:\n",
        "      stddev = np.sqrt(6) / (np.sqrt(n_in))\n",
        "      return self._truncated_normal(n_in, n_out, stddev)\n",
        " \n",
        "  def _uniform(self, n_in, n_out, bounds):\n",
        "    W = np.random.uniform(\n",
        "        low=-bounds,\n",
        "        high=bounds,\n",
        "        size=(n_in, n_out)\n",
        "      )\n",
        "    return W\n",
        "  \n",
        "  def _truncated_normal(self, n_in, n_out, stddev):\n",
        "    W = np.random.normal(\n",
        "        loc=0,\n",
        "        scale=stddev,\n",
        "        size=(n_in, n_out)\n",
        "      )\n",
        "    #truncate results - anything > 2 stddev out gets clipped\n",
        "    W[W> 2*stddev] = 2*stddev\n",
        "    W[W<-2*stddev] = -2*stddev\n",
        "    return W\n",
        "  \n",
        "  def __init__(self, init_method=\"xavier\"):\n",
        "    if init_method==\"xavier\":\n",
        "      self.f = self.xavier\n",
        "    elif init_method==\"he\":\n",
        "      self.f = self.he"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "coc2QCMsn63E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calc_MSE(y, y_hat):\n",
        "  error = y-y_hat\n",
        "  return np.mean(np.sum(error**2, axis=1))\n",
        "\n",
        "def labels_from_preds(preds):\n",
        "  return np.argmax(preds, axis=1)\n",
        "\n",
        "def calc_accuracy(labels, target):\n",
        "  return np.sum(labels == target) / len(target)\n",
        "\n",
        "#wasn't sure if we could use a package to shuffle so found this code: https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
        "def shuffle_in_unison(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
        "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
        "    permutation = np.random.permutation(len(a))\n",
        "    for old_index, new_index in enumerate(permutation):\n",
        "        shuffled_a[new_index] = a[old_index]\n",
        "        shuffled_b[new_index] = b[old_index]\n",
        "    return shuffled_a, shuffled_b\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JbRDaYgyBsh8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The function for ReLU\n",
        "\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    x & \\mbox{if } x > 0 \\\\\n",
        "    0 & \\mbox{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "The function for ReLU's derivative\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    1 & \\mbox{if } x > 0 \\\\\n",
        "    0 & \\mbox{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "\n",
        "The function for Leaky ReLU\n",
        "\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    x & \\mbox{if } x > 0 \\\\\n",
        "    0.01x & \\mbox{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "The function for ReLU's derivative\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    1 & \\mbox{if } x > 0 \\\\\n",
        "    0.01 & \\mbox{otherwise}\n",
        "\\end{cases}$\n"
      ]
    },
    {
      "metadata": {
        "id": "kYXDLyEWGG2r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "class Activation(object):\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def tanh_deriv(self, a):\n",
        "        # a = np.tanh(x)   \n",
        "        return 1.0 - a**2\n",
        "    def logistic(self, x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    def logistic_deriv(self, a):\n",
        "        # a = logistic(x) \n",
        "        return  a * (1 - a )\n",
        "      \n",
        "    def ReLU(self, x):\n",
        "        x[x<0] =0\n",
        "        return x\n",
        "      \n",
        "    def ReLU_deriv(self, a):\n",
        "        der = np.zeros(a.shape)\n",
        "        der[a>0] =1\n",
        "        return der\n",
        "      \n",
        "    def leaky_ReLU(self, x):\n",
        "        x = np.where(x > 0, x, x*0.01)\n",
        "        return x\n",
        "      \n",
        "    def leaky_ReLU_deriv(self, a):\n",
        "        der = np.full(a.shape, 0.01)\n",
        "        der[a>0] =1\n",
        "        return der\n",
        "      \n",
        "    def softmax(self, x):\n",
        "        # apply max normalization to avoid overflow\n",
        "        if len(x.shape) > 1:\n",
        "          x_norm = (x.T - np.max(x, axis=1)).T\n",
        "          return softmax(x_norm, axis=1)\n",
        "        else:\n",
        "          x_norm = x - np.max(x)\n",
        "          return softmax(x_norm)\n",
        "      \n",
        "    def softmax_deriv(self, a):\n",
        "        return np.ones(a.shape)\n",
        "    \n",
        "    def __init__(self,activation='tanh'):\n",
        "        if activation == 'logistic':\n",
        "            self.f = self.logistic\n",
        "            self.f_deriv = self.logistic_deriv\n",
        "        elif activation == 'tanh':\n",
        "            self.f = self.tanh\n",
        "            self.f_deriv = self.tanh_deriv\n",
        "        elif activation == 'relu':\n",
        "            self.f = self.ReLU\n",
        "            self.f_deriv = self.ReLU_deriv\n",
        "        elif activation == 'leaky_relu':\n",
        "            self.f = self.leaky_ReLU\n",
        "            self.f_deriv = self.leaky_ReLU_deriv\n",
        "        elif activation == 'softmax':\n",
        "            self.f = self.softmax\n",
        "            self.f_deriv = self.softmax_deriv\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HbjWqZ24L3Ot",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Loss_function(object):\n",
        "    def MSE(self, y, y_hat):\n",
        "        error = y-y_hat\n",
        "        loss=np.sum(error**2)\n",
        "        return loss\n",
        "      \n",
        "    def Cross_entropy(self, y, y_hat):\n",
        "        return -np.log(y_hat[np.argmax(y)])\n",
        "      \n",
        "    def l2_reg(self, reg_weight, layers, sample_weight):\n",
        "        accum = 0\n",
        "        for layer in layers:\n",
        "          accum += np.sum(np.square(layer.W))\n",
        "          \n",
        "        return accum*reg_weight*sample_weight/2\n",
        "        \n",
        "    def __init__(self,loss='cross_entropy'):\n",
        "        if loss == 'MSE':\n",
        "            self.loss = self.MSE\n",
        "        elif loss == 'cross_entropy':\n",
        "            self.loss = self.Cross_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BeoakGoCGLvb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class HiddenLayer(object):    \n",
        "    def __init__(self,n_in, n_out,\n",
        "                 activation_last_layer='tanh',activation='tanh', W=None, b=None,\n",
        "                init_uniform=True, weight_decay=None):\n",
        "        \"\"\"\n",
        "        Typical hidden layer of a MLP: units are fully-connected and have\n",
        "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
        "        and the bias vector b is of shape (n_out,).\n",
        "\n",
        "        NOTE : The nonlinearity used here is tanh\n",
        "\n",
        "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
        "\n",
        "        :type n_in: int\n",
        "        :param n_in: dimensionality of input\n",
        "\n",
        "        :type n_out: int\n",
        "        :param n_out: number of hidden units\n",
        "\n",
        "        :type activation: string\n",
        "        :param activation: Non linearity to be applied in the hidden\n",
        "                           layer\n",
        "        :type init_uniform: bool\n",
        "        :param init_uniform: Whether to draw init weights from uniform dist (else normal)\n",
        "        \n",
        "        :type weight_decay: float/None/False\n",
        "        :param weight_decay: Weight to apply to l2 reg loss factor (else none/false)\n",
        "        \"\"\"\n",
        "        self.input=None\n",
        "        self.activation=Activation(activation).f\n",
        "        \n",
        "        if activation.contains('relu'):\n",
        "          self.init_weights = InitWeights(\"he\").f\n",
        "        else:\n",
        "          self.init_weights = InitWeights(\"xavier\").f\n",
        "        \n",
        "        # activation deriv of last layer\n",
        "        self.activation_deriv=None\n",
        "        if activation_last_layer:\n",
        "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
        "\n",
        "        if W is not None:\n",
        "          self.W = W\n",
        "        else:\n",
        "          self.W = self.init_weights(n_in, n_out, init_uniform)\n",
        "\n",
        "        if b is not None:\n",
        "          self.b = b\n",
        "        else:\n",
        "          self.b = np.zeros(n_out,)  \n",
        "          \n",
        "        self.weight_decay = weight_decay\n",
        "          \n",
        "        self.grad_W = np.zeros(self.W.shape)\n",
        "        self.grad_b = np.zeros(self.b.shape)\n",
        "        \n",
        "        # create arrays to store the velocity values for momentum calculation\n",
        "        self.vW = np.zeros(self.W.shape)\n",
        "        self.vb = np.zeros(self.b.shape)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        :type input: numpy.array\n",
        "        :param input: a symbolic tensor of shape (n_in,)\n",
        "        '''\n",
        "        lin_output = np.dot(input, self.W) + self.b\n",
        "        self.output = (\n",
        "            lin_output if self.activation is None\n",
        "            else self.activation(lin_output)\n",
        "        )\n",
        "        self.input=input\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, delta, output_layer=False, sampleweight=1):\n",
        "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
        "        \n",
        "        if self.weight_decay:\n",
        "          self.grad_W += self.W * self.weight_decay * sampleweight\n",
        "        self.grad_b = np.sum(delta, axis=0)\n",
        "        if self.activation_deriv:\n",
        "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
        "        return delta\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_LKgiGhdGQbS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class MLP:\n",
        "    \"\"\"\n",
        "    \"\"\"      \n",
        "    def __init__(self, layers=None, activation=[None,'tanh','tanh'], init_uniform=True, weight_decay=False, from_file=None):\n",
        "        \"\"\"\n",
        "        :param layers: A list containing the number of units in each layer.\n",
        "        Should be at least two values\n",
        "        :param activation: The activation function to be used. Can be\n",
        "        \"logistic\" or \"tanh\"\n",
        "        :param init_uniform: Whether to draw init weights from uniform dist (else normal)\n",
        "        :param weight_decay: lambda for strength of l2 regularization on weights (else False/None for no reg)\n",
        "        :param from_file: a file to load to get pretrained weights. \n",
        "        \"\"\"        \n",
        "        ### initialize layers\n",
        "        self.layers=[]\n",
        "        self.params= {'activation':activation, 'layers':layers, 'weight_decay': weight_decay, 'init_uniform': init_uniform}\n",
        "        \n",
        "        if from_file:\n",
        "          dumped_model = self._load_model(from_file)\n",
        "          self.params = dumped_model['params']\n",
        "          self.activation=self.params['activation']\n",
        "          layers = self.params['layers']\n",
        "          init_uniform = self.params['init_uniform']\n",
        "          for i in range(len(self.params['layers'])-1):\n",
        "              self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],  \n",
        "                                             W=dumped_model['weights'][i][0], b=dumped_model['weights'][i][1], weight_decay=weight_decay, init_uniform=init_uniform))\n",
        "        else:\n",
        "          self.activation=activation\n",
        "          for i in range(len(layers)-1):\n",
        "              self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1], weight_decay=weight_decay, init_uniform=init_uniform))\n",
        "            \n",
        "    def forward(self,input):\n",
        "        for layer in self.layers:\n",
        "            output=layer.forward(input)\n",
        "            input=output\n",
        "        return output\n",
        "      \n",
        "    def calculate_loss(self,y,y_hat):\n",
        "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
        "        # call to loss function\n",
        "        loss=[]\n",
        "        delta=[]\n",
        "\n",
        "        for i, single_y in enumerate(y):\n",
        "          loss.append(Loss_function('MSE').loss(single_y, y_hat[i]))\n",
        "          error = single_y-y_hat[i]\n",
        "        # calculate the delta of the output layer\n",
        "          delta.append(np.array(-error*activation_deriv(y_hat[i])))\n",
        "        # return loss and delta\n",
        "        loss = np.array(loss)\n",
        "        if self.params['weight_decay']:\n",
        "          loss += Loss_function().l2_reg(self.params['weight_decay'], self.layers, len(y)/self.Xcount)\n",
        "\n",
        "        return loss,np.array(delta)\n",
        "        \n",
        "    def backward(self,delta, sampleweight):\n",
        "        delta=self.layers[-1].backward(delta,output_layer=True, sampleweight=sampleweight)\n",
        "        for layer in reversed(self.layers[:-1]):\n",
        "            delta=layer.backward(delta, sampleweight=sampleweight)\n",
        "            \n",
        "    def update(self,lr):\n",
        "        for layer in self.layers:\n",
        "            layer.W -= lr * layer.grad_W\n",
        "            layer.b -= lr * layer.grad_b\n",
        "            \n",
        "    def update_momentum(self, lr, mom):\n",
        "        for layer in self.layers:\n",
        "            layer.vW = mom * layer.vW + lr * layer.grad_W\n",
        "            layer.vb = mom * layer.vb + lr * layer.grad_b\n",
        "            layer.W -= layer.vW\n",
        "            layer.b -= layer.vb        \n",
        "\n",
        "    def fit(self,X,y,learning_rate=0.1, epochs=100):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param X: Input data or features\n",
        "        :param y: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        \"\"\" \n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs)\n",
        "        self.Xcount = len(X)\n",
        "        for k in range(epochs):\n",
        "            #print('epoch', k)\n",
        "            loss=np.zeros(X.shape[0])\n",
        "            for it in range(X.shape[0]):\n",
        "                i=np.random.randint(X.shape[0])\n",
        "                \n",
        "                # forward pass\n",
        "                y_hat = self.forward(X[i])\n",
        "                # backward pass\n",
        "                loss[it],delta=self.calculate_loss([y[i]],[y_hat])\n",
        "                self.backward(delta, 1/self.Xcount)\n",
        "                # update\n",
        "                self.update(learning_rate)\n",
        "            to_return[k] = np.mean(loss)\n",
        "            if not k % 10:\n",
        "              print(\".\", end=\"\")\n",
        "        return to_return\n",
        "      \n",
        "    def fit_mb(self,X,y,mini_batch_size,learning_rate=0.1, epochs=100):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param X: Input data or features\n",
        "        :param y: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        \"\"\" \n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs) #array to store values of mean loss for each epoch for plotting later\n",
        "        self.Xcount = len(X)\n",
        "        \n",
        "        for k in range(epochs): #for each epoch\n",
        "            X, y = shuffle_in_unison(X, y) #shuffle the input data and input targets\n",
        "            loss=np.zeros(X.shape[0]) #create array of zeros whose lengths = #samples.\n",
        "            \n",
        "            #partition training data (X, y) into mini-batches\n",
        "            for j in range(0, X.shape[0], mini_batch_size):\n",
        "              X_mini = X[j:j + mini_batch_size]\n",
        "              y_mini = y[j:j + mini_batch_size]\n",
        "              # forward pass\n",
        "              y_hat = self.forward(X_mini) #forward feed the mini_batches to get outputs (y_hat)\n",
        "              \n",
        "              # backwards pass\n",
        "              loss[j:j + mini_batch_size], delta=self.calculate_loss(y[j:j + mini_batch_size], y_hat) #input y and y_hat into calculate_loss. Output = loss and delta\n",
        "              self.backward(delta, mini_batch_size/self.Xcount) #pass delta from calculate_loss to backward.\n",
        "\n",
        "              # update\n",
        "              self.update(learning_rate)\n",
        "            to_return[k] = np.mean(loss) #add mean loss to to_return\n",
        "            if not k % 10:\n",
        "              print(\".\", end=\"\")\n",
        "        return to_return\n",
        "      \n",
        "    def fit_SGD_momentum(self,X,y,learning_rate=0.1, epochs=100, momentum=0.9):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param X: Input data or features\n",
        "        :param y: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        \"\"\" \n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs)\n",
        "        self.Xcount = len(X)\n",
        "        for k in range(epochs):\n",
        "            loss=np.zeros(X.shape[0])\n",
        "            \n",
        "            # loop through training examples\n",
        "            for j in range(X.shape[0]):\n",
        "              i=np.random.randint(X.shape[0])\n",
        "                \n",
        "              # forward pass\n",
        "              y_hat = self.forward(X[i])\n",
        "                \n",
        "              # backward pass\n",
        "              loss[j],delta=self.calculate_loss([y[i]],[y_hat])\n",
        "              self.backward(delta, X.shape[0]/self.Xcount)\n",
        "                \n",
        "              # update\n",
        "              self.update_momentum(learning_rate, momentum)\n",
        "            to_return[k] = np.mean(loss)\n",
        "            if not k % 10:\n",
        "              print(\".\", end=\"\")\n",
        "        return to_return  \n",
        "\n",
        "    def predict(self, x):\n",
        "        x = np.array(x)\n",
        "        output = []\n",
        "        for i in np.arange(x.shape[0]):\n",
        "            output.append(self.forward(x[i,:]))\n",
        "        return np.vstack(output)\n",
        "      \n",
        "\n",
        "    def save_model(self, name):\n",
        "      model = {'params':self.params, 'weights':[]}\n",
        "      for x in self.layers:\n",
        "        model['weights'].append((x.W, x.b))\n",
        "        \n",
        "      with open(model_dir.format(name), 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "        \n",
        "    def _load_model(self, name):\n",
        "      with open(model_dir.format(name), 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOPAt_9nGd5y",
        "colab_type": "code",
        "outputId": "e149909e-d7f7-4571-96ca-9700037e1579",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "np.seterr(all=\"warn\")\n",
        "np.random.seed(1)\n",
        "procdata = np.copy(data)\n",
        "preprocess(procdata, 'zscore')\n",
        "\n",
        "#split data\n",
        "train, train_target, validate, validate_target = split(procdata, label)\n",
        "#one hot encode targets\n",
        "train_target = OHE(train_target, 10)\n",
        "validate_target = OHE(validate_target, 10)\n",
        "second_layer = False\n",
        "relu = True\n",
        "if second_layer:\n",
        "  nn = MLP([128,60,30,10], [None,'logistic','logistic','tanh'])\n",
        "elif relu:\n",
        "  nn = MLP([128,60,10, 10], [None, 'relu','relu', 'softmax'])\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb(train, train_target, learning_rate=0.001, epochs=500, mini_batch_size=32)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "else:\n",
        "  nn = MLP([128,60,10], [None,'logistic','tanh'], init_uniform=False, weight_decay=0.5)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb(train, train_target, learning_rate=0.01, epochs=500, mini_batch_size=32)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "........."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-U-Uf7i9YX9t",
        "colab_type": "code",
        "outputId": "75f99a89-0828-4911-c578-56ad35674846",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "pl.figure(figsize=(15,4))\n",
        "pl.plot(MSE)\n",
        "pl.grid()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3QAAAD8CAYAAAA/m+aTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJYpCBwAAAAAlikIHAAAAACXq/wMX5F7VVvWMwQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ms28tiW02uzz",
        "colab_type": "code",
        "outputId": "fd14e34a-3fb8-4d10-8640-900c8d69b3b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#get validation score\n",
        "#nn = load_model(\"tdm1\")\n",
        "preds = nn.predict(validate)\n",
        "\n",
        "loss = calc_MSE(preds, validate_target)\n",
        "loss"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.17554347345263557"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "Chank-E-im1p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nn.save_model(\"relu.1\")\n",
        "\n",
        "calc_accuracy(labels_from_preds(preds), labels_from_preds(validate_target)) # we have to de-OHE the predictions and the target data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VPIf-BuaBzlC",
        "colab_type": "code",
        "outputId": "98a8f038-23c9-49ed-f484-39ca773a86b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.mean(procdata)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.4332387956368016e-19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "metadata": {
        "id": "CauivsamljjX",
        "colab_type": "code",
        "outputId": "7a18fa17-b144-4f88-d7e1-7368a8fb4d2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "cell_type": "code",
      "source": [
        "np.seterr(all=\"warn\")\n",
        "np.random.seed(1)\n",
        "procdata = np.copy(data)\n",
        "preprocess(procdata, 'zscore')\n",
        "\n",
        "#split data\n",
        "train, train_target, validate, validate_target = split(data, label)\n",
        "#one hot encode targets\n",
        "train_target = OHE(train_target, 10)\n",
        "validate_target = OHE(validate_target, 10)\n",
        "second_layer = False\n",
        "relu = False\n",
        "if second_layer:\n",
        "  nn = MLP([128,60,30,10], [None,'logistic','logistic','tanh'])\n",
        "elif relu:\n",
        "  nn = MLP([128,60,10], [None, 'relu', 'relu'], False)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_SGD_momentum(train, train_target, learning_rate=0.001, epochs=25)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "else:\n",
        "  nn = MLP([128,60,10], [None,'logistic','tanh'], False)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_SGD_momentum(train, train_target, learning_rate=0.01, epochs=25)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: overflow encountered in exp\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: underflow encountered in exp\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: underflow encountered in true_divide\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: RuntimeWarning: underflow encountered in multiply\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: RuntimeWarning: underflow encountered in multiply\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: RuntimeWarning: underflow encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "."
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-a0a50fdb1566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'logistic'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_SGD_momentum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}s to train\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss:%f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mMSE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-17c3f39b9e9f>\u001b[0m in \u001b[0;36mfit_SGD_momentum\u001b[0;34m(self, X, y, learning_rate, epochs, momentum)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;31m# loop through training examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m               \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m               \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "XAmMCUzSupSo",
        "colab_type": "code",
        "outputId": "dc504048-9388-4cd2-904a-e54dc3b7f1bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "cell_type": "code",
      "source": [
        "pl.figure(figsize=(15,4))\n",
        "pl.plot(MSE)\n",
        "pl.grid()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2gAAAD4CAYAAACZmMXjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "HExGAmdDalm0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Introduction\n",
        "The experiment task consisted of building a neural network to perform multi-class classification on a supplied dataset without the use of Deep Learning frameworks (e.g. TensorFlow, Caffe, and KERAS). The dataset consisted of 60,000 labeled training samples and 10,000 unlabeled test samples. The structure of the data (e.g. image, video, etc) was unknown. The performance of the neural network was evaluated in terms of the accuracy metric. Various neural network structures and parameters were trailed to maximise speed and accuracy.\n",
        "\n",
        "The objective of building the neural network without Deep Learning frameworks was to gain a comprehensive understanding of the math and mechanics behind neural networks.\n",
        "\n",
        "\n",
        "\n",
        "##SGD with Momentum\n",
        "Momentum ($v_t$) is an exponentially weighted average of a neural networks gradients. It is used to update the weights ($w_t$) and baises ($b_t$) of a network.\n",
        "\n",
        "$$v_t = \\beta v_{t-1} + \\eta \\nabla_w J(w)$$\n",
        "$$w_t = w_{t-1} - v_t$$\n",
        "\n",
        "Momentum increases for features whose gradients point in the same direction and reduces for features whose gradients change direction. By reducing the fluctuation of gradients convergence is generally sped up. The hyper-parameter $\\beta$ takes a value between 0 - 1 and dictates how many samples are included in the exponential weighted average. A small $\\beta$ value will increase fluctuation because the average is taken over a smaller number of examples. A large $\\beta$ will increase smoothing because the average is taken over a larger number of examples. A $\\beta$ value of 0.9 provides a balance between the two extremes.\n",
        "\n",
        "##Gradient Descent\n",
        "Gradient descent is a machine learning optimization method. In deep learning it is used to calculate the model parameters (weights and biases) that minimise the cost function. The gradient descent method invovles iterating through a training dataset and updating weights and baises in accordance with the gradient of error. There are three types of gradient descent. Each uses a different number of training examples to update the model parameters:\n",
        "*   **Batch Gradient Descent** uses the entire training dataset to calculate gradients and update the parameters. Because the entire training dataset is considered parameters updates are smooth however, it can take a long time to make a single update.\n",
        "*   **Stochastic Gradient Descent (SGD)** uses a single randomly selected sample from the training dataset to calculate gradients and update the parameters. Parameter updates are fast but very noisey.\n",
        "*   **Mini-batch Gradient Descent** uses a subset of the training data (e.g. batches of 1000 samples) to calculate gradients and update the parameters. Mini-batch gradient descent is a compromise between batch and stochastic gradient descent. The mini-batch size can be adjusted to find the appropriate balance between fast convergence and noisey updates. "
      ]
    },
    {
      "metadata": {
        "id": "BRDGNA5IPpGY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Louis testing stuff below"
      ]
    },
    {
      "metadata": {
        "id": "WLXWBGImOPVM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.seterr(all=\"warn\")\n",
        "np.random.seed(1)\n",
        "procdata = np.copy(data)\n",
        "preprocess(procdata, 'zscore')\n",
        "\n",
        "#split data\n",
        "train, train_target, validate, validate_target = split(data, label)\n",
        "#one hot encode targets\n",
        "train_target = OHE(train_target, 10)\n",
        "validate_target = OHE(validate_target, 10)\n",
        "second_layer = False\n",
        "relu = False\n",
        "\n",
        "nn = MLP([128,60,10], [None, 'tanh', 'tanh'], False)\n",
        "start = time.time()\n",
        "MSE = nn.fit(train, train_target, learning_rate=0.001, epochs=5)\n",
        "# MSE = nn.fit_mb(train, train_target, 500, learning_rate=0.001, epochs=10)\n",
        "print(\"{}s to train\".format(time.time() - start))\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dv2bA_5yPoPg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "np.seterr(all=\"warn\")\n",
        "np.random.seed(1)\n",
        "procdata = np.copy(data)\n",
        "preprocess(procdata, 'zscore')\n",
        "\n",
        "#split data\n",
        "train, train_target, validate, validate_target = split(data, label)\n",
        "#one hot encode targets\n",
        "train_target = OHE(train_target, 10)\n",
        "validate_target = OHE(validate_target, 10)\n",
        "second_layer = False\n",
        "relu = False\n",
        "if second_layer:\n",
        "  nn = MLP([128,60,30,10], [None,'logistic','logistic','tanh'])\n",
        "elif relu:\n",
        "  nn = MLP([128,60,10], [None, 'relu', 'relu'], False)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit(train, train_target, learning_rate=0.001, epochs=500)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "else:\n",
        "  nn = MLP([128,60,10], [None,'logistic','tanh'], init_uniform=False, weight_decay=0.1)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb(train, train_target,20, learning_rate=0.01, epochs=5)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3fVa6fliErm5",
        "colab_type": "code",
        "outputId": "5f8091e2-23fc-41cf-973c-8b072e42e9a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "cell_type": "code",
      "source": [
        "pl.figure(figsize=(15,4))\n",
        "pl.plot(MSE)\n",
        "pl.grid()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3MAAAD4CAYAAABG1r/8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAwAAKND/AXp9WANlcn53AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "0C-vQveSFXUZ",
        "colab_type": "code",
        "outputId": "fe686dd4-5a00-4bfa-a37d-c600cb50bcf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "preds = nn.predict(validate)\n",
        "calc_accuracy(labels_from_preds(preds), labels_from_preds(validate_target))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in greater\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "metadata": {
        "id": "oGB6TzkSUpZE",
        "colab_type": "code",
        "outputId": "5cdf467b-f2fd-463b-e309-386f2f8e8340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([[1,2], [3,4]])\n",
        "w = np.array([[1,2], [3,4]])\n",
        "d = np.array([1,2])\n",
        "print(np.dot(x, w)+d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 8 12]\n",
            " [16 24]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hQyt7v35VPJs",
        "colab_type": "code",
        "outputId": "6fad152f-8009-4e32-a3a2-7addb829fcb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([1,2])\n",
        "w = np.array([[1,2], [3,4]])\n",
        "d = np.array([1,2])\n",
        "print(np.dot(x, w)+d)\n",
        "x = np.array([3,4])\n",
        "print(np.dot(x, w)+d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 8 12]\n",
            "[16 24]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}