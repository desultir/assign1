{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desultir/assign1/blob/master/MNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fQvPcbXfGAnt",
        "colab_type": "code",
        "outputId": "481742c7-de57-4bc3-b8db-d3a33f3c192e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scipy\n",
        "!pip install --upgrade numpy\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scipy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/5f/c48860704092933bf1c4c1574a8de1ffd16bf4fde8bab190d747598844b2/scipy-1.2.1-cp36-cp36m-manylinux1_x86_64.whl (24.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 24.8MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.14.6)\n",
            "\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mfastai 1.0.49 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "  Found existing installation: scipy 1.1.0\n",
            "    Uninstalling scipy-1.1.0:\n",
            "      Successfully uninstalled scipy-1.1.0\n",
            "Successfully installed scipy-1.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "scipy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 17.3MB 2.1MB/s \n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mdatascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.14.6\n",
            "    Uninstalling numpy-1.14.6:\n",
            "      Successfully uninstalled numpy-1.14.6\n",
            "Successfully installed numpy-1.16.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "GGkj2RPTaJ9g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as pl\n",
        "from ipywidgets import interact, widgets\n",
        "from matplotlib import animation\n",
        "import h5py\n",
        "from google.colab import drive\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kf3k_uTGGlqK",
        "colab_type": "code",
        "outputId": "d1a19eef-c8cb-48ef-b65a-341cc09974ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "with h5py.File('/content/drive/My Drive/Colab Notebooks/Input/train_128.h5','r') as H: \n",
        "  data = np.copy(H['data'])\n",
        "with h5py.File('/content/drive/My Drive/Colab Notebooks/Input/train_label.h5','r') as H:\n",
        "  label = np.copy(H['label'])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DOSt_vG94Mfi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# need to normalize input data to avoid overflow/underflow in initial epochs\n",
        "# normalize each feature independently\n",
        "# options are zscore, minmax\n",
        "def preprocess(input_array, method='zscore'):\n",
        "  if method == 'zscore':\n",
        "    for i in range(input_array.shape[1]):\n",
        "      mean = np.mean(input_array[:, i])\n",
        "      std = np.std(input_array[:, i])\n",
        "      input_array[:, i] = (input_array[:, i] - mean) / std\n",
        "  elif method == 'minmax':\n",
        "    for i in range(input_array.shape[1]):\n",
        "      # range 0 to max\n",
        "      input_array[:, i] = (input_array[:, i] - np.min(input_array[:, i]))\n",
        "      # range 0 to 2\n",
        "      input_array[:, i] /= (np.max(input_array[:, i]) / 2)\n",
        "      # range -1 to 1\n",
        "      input_array[:, i] -= 1\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d50vSdHPfzVT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "611e695f-976b-4b63-a015-63fdaad58a36"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "len(np.random.choice(np.ravel(np.argwhere(label == 9)), 1000))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "hCFYVtU06MPR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#use stratified sampling to split train into train/validation\n",
        "#this dataset is actually balanced but still good practice\n",
        "def split(dataset, labels, train_percent=.85):\n",
        "  count = len(dataset)\n",
        "  num_classes = np.max(label) + 1\n",
        "  train = []\n",
        "  train_target = []\n",
        "  validate = []\n",
        "  validate_target = []\n",
        "  for i in range(num_classes):\n",
        "    class_data = np.ravel(np.argwhere(label == i))\n",
        "    np.random.shuffle(class_data)\n",
        "    cutoff = int(len(class_data) * train_percent)\n",
        "    train_idx = class_data[:cutoff]\n",
        "    val_idx = class_data[cutoff:]\n",
        "    train.append(dataset[train_idx])\n",
        "    train_target.append(labels[train_idx])\n",
        "    validate.append(dataset[val_idx])\n",
        "    validate_target.append(labels[val_idx])\n",
        "    \n",
        "  return np.vstack(train), np.hstack(train_target), np.vstack(validate), np.hstack(validate_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RsZmOeyySQq9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#need to one-hot encode labels to map to N output nodes (1 per class)\n",
        "#ie convert each label into a (10,) vector where the relevant column is 1\n",
        "\n",
        "def OHE(input_array, num_classes=10):\n",
        "  output = []\n",
        "  for x in input_array:\n",
        "    output.append(np.zeros((10,)))\n",
        "    output[-1][x] = 1\n",
        "  return np.vstack(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "coc2QCMsn63E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def MSE(y, y_hat):\n",
        "  error = y-y_hat\n",
        "  return np.sum(error**2)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kYXDLyEWGG2r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Activation(object):\n",
        "    def __tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def __tanh_deriv(self, a):\n",
        "        # a = np.tanh(x)   \n",
        "        return 1.0 - a**2\n",
        "    def __logistic(self, x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    def __logistic_deriv(self, a):\n",
        "        # a = logistic(x) \n",
        "        return  a * (1 - a )\n",
        "    \n",
        "    def __init__(self,activation='tanh'):\n",
        "        if activation == 'logistic':\n",
        "            self.f = self.__logistic\n",
        "            self.f_deriv = self.__logistic_deriv\n",
        "        elif activation == 'tanh':\n",
        "            self.f = self.__tanh\n",
        "            self.f_deriv = self.__tanh_deriv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BeoakGoCGLvb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class HiddenLayer(object):    \n",
        "    def __init__(self,n_in, n_out,\n",
        "                 activation_last_layer='tanh',activation='tanh', W=None, b=None):\n",
        "        \"\"\"\n",
        "        Typical hidden layer of a MLP: units are fully-connected and have\n",
        "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
        "        and the bias vector b is of shape (n_out,).\n",
        "\n",
        "        NOTE : The nonlinearity used here is tanh\n",
        "\n",
        "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
        "\n",
        "        :type n_in: int\n",
        "        :param n_in: dimensionality of input\n",
        "\n",
        "        :type n_out: int\n",
        "        :param n_out: number of hidden units\n",
        "\n",
        "        :type activation: string\n",
        "        :param activation: Non linearity to be applied in the hidden\n",
        "                           layer\n",
        "        \"\"\"\n",
        "        self.input=None\n",
        "        self.activation=Activation(activation).f\n",
        "        \n",
        "        # activation deriv of last layer\n",
        "        self.activation_deriv=None\n",
        "        if activation_last_layer:\n",
        "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
        "\n",
        "        self.W = np.random.normal(\n",
        "                loc=0,\n",
        "                scale=1,\n",
        "                size=(n_in, n_out)\n",
        "        )\n",
        "        self.W *= 1/n_in\n",
        "        if activation == 'relu':\n",
        "            #relu\n",
        "            self.W *= 2\n",
        "\n",
        "        self.b = np.zeros(n_out,)\n",
        "        \n",
        "        self.grad_W = np.zeros(self.W.shape)\n",
        "        self.grad_b = np.zeros(self.b.shape)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        :type input: numpy.array\n",
        "        :param input: a symbolic tensor of shape (n_in,)\n",
        "        '''\n",
        "        lin_output = np.dot(input, self.W) + self.b\n",
        "        self.output = (\n",
        "            lin_output if self.activation is None\n",
        "            else self.activation(lin_output)\n",
        "        )\n",
        "        self.input=input\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, delta, output_layer=False):\n",
        "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
        "        self.grad_b = delta\n",
        "        if self.activation_deriv:\n",
        "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
        "        return delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_LKgiGhdGQbS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    \"\"\"\n",
        "    \"\"\"      \n",
        "    def __init__(self, layers, activation=[None,'tanh','tanh']):\n",
        "        \"\"\"\n",
        "        :param layers: A list containing the number of units in each layer.\n",
        "        Should be at least two values\n",
        "        :param activation: The activation function to be used. Can be\n",
        "        \"logistic\" or \"tanh\"\n",
        "        \"\"\"        \n",
        "        ### initialize layers\n",
        "        self.layers=[]\n",
        "        self.params=[]\n",
        "        \n",
        "        self.activation=activation\n",
        "        for i in range(len(layers)-1):\n",
        "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1]))\n",
        "    def forward(self,input):\n",
        "        for layer in self.layers:\n",
        "            output=layer.forward(input)\n",
        "            input=output\n",
        "        return output\n",
        "    def criterion_MSE(self,y,y_hat):\n",
        "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
        "        # MSE\n",
        "        error = y-y_hat\n",
        "        loss=np.sum(error**2)\n",
        "        # calculate the delta of the output layer\n",
        "        delta=-error*activation_deriv(y_hat)    \n",
        "        # return loss and delta\n",
        "        return loss,delta\n",
        "        \n",
        "    def backward(self,delta):\n",
        "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
        "        for layer in reversed(self.layers[:-1]):\n",
        "            delta=layer.backward(delta)\n",
        "            \n",
        "    def update(self,lr):\n",
        "        for layer in self.layers:\n",
        "            layer.W -= lr * layer.grad_W\n",
        "            layer.b -= lr * layer.grad_b\n",
        "\n",
        "    def fit(self,X,y,learning_rate=0.1, epochs=100):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param X: Input data or features\n",
        "        :param y: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        \"\"\" \n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs)\n",
        "        \n",
        "        for k in range(epochs):\n",
        "            loss=np.zeros(X.shape[0])\n",
        "            for it in range(X.shape[0]):\n",
        "                i=np.random.randint(X.shape[0])\n",
        "                \n",
        "                # forward pass\n",
        "                y_hat = self.forward(X[i])\n",
        "                \n",
        "                # backward pass\n",
        "                loss[it],delta=self.criterion_MSE(y[i],y_hat)\n",
        "                self.backward(delta)\n",
        "                \n",
        "                # update\n",
        "                self.update(learning_rate)\n",
        "            to_return[k] = np.mean(loss)\n",
        "            if not k % 10:\n",
        "              print(\".\", end=\"\")\n",
        "        return to_return\n",
        "\n",
        "    def predict(self, x):\n",
        "        x = np.array(x)\n",
        "        output = []\n",
        "        for i in np.arange(x.shape[0]):\n",
        "            output.append(nn.forward(x[i,:]))\n",
        "        return np.vstack(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOPAt_9nGd5y",
        "colab_type": "code",
        "outputId": "562c631a-680f-4d69-e2e9-9f8ee21464cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "np.seterr(all=\"warn\")\n",
        "np.random.seed(1)\n",
        "procdata = np.copy(data)\n",
        "preprocess(procdata, 'zscore')\n",
        "\n",
        "#split data\n",
        "train, train_target, validate, validate_target = split(data, label)\n",
        "#one hot encode targets\n",
        "train_target = OHE(train_target, 10)\n",
        "validate_target = OHE(validate_target, 10)\n",
        "\n",
        "nn = MLP([128,60,10], [None,'logistic','tanh'])\n",
        "start = time.time()\n",
        "MSE = nn.fit(train, train_target, learning_rate=0.01, epochs=500)\n",
        "print(\"{}s to train\".format(time.time() - start))\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: underflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: underflow encountered in true_divide\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:64: RuntimeWarning: underflow encountered in multiply\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: RuntimeWarning: underflow encountered in multiply\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: underflow encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-U-Uf7i9YX9t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pl.figure(figsize=(15,4))\n",
        "pl.plot(MSE)\n",
        "pl.grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ms28tiW02uzz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#get validation score\n",
        "preds = nn.predict(validate)\n",
        "\n",
        "loss = MSE(preds, validate_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Chank-E-im1p",
        "colab_type": "code",
        "outputId": "718c280c-f972-44e7-b29b-09a23e5b78d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "for x in nn.layers:\n",
        "  print(np.max(x.W))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12.227258419248177\n",
            "0.42391281028297456\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}