{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desultir/assign1/blob/master/MNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fQvPcbXfGAnt",
        "colab_type": "code",
        "outputId": "5f8deca1-1c5c-4b37-c5f3-f1da36662252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scipy\n",
        "!pip install --upgrade numpy\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: scipy in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.16.3)\n",
            "Requirement already up-to-date: numpy in /usr/local/lib/python3.6/dist-packages (1.16.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GGkj2RPTaJ9g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as pl\n",
        "from ipywidgets import interact, widgets\n",
        "from matplotlib import animation\n",
        "import h5py\n",
        "from google.colab import drive\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kf3k_uTGGlqK",
        "colab_type": "code",
        "outputId": "3c5e2a24-e3a1-4ee2-d98d-98d74922aec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "with h5py.File('/content/drive/My Drive/Colab Notebooks/Input/train_128.h5','r') as H:\n",
        "  data = np.copy(H['data'])\n",
        "with h5py.File('/content/drive/My Drive/Colab Notebooks/Input/train_label.h5','r') as H:\n",
        "  label = np.copy(H['label'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rJ4v9obPcy13",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import pickle\n",
        "import glob\n",
        "\n",
        "model_dir = \"/content/drive/My Drive/Colab Notebooks/Models/{}.pk\"\n",
        "\n",
        "def list_models():\n",
        "  return glob.glob(model_dir.format(\"*\"))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOSt_vG94Mfi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# need to normalize input data to avoid overflow/underflow in initial epochs\n",
        "# normalize each feature independently\n",
        "# options are zscore, minmax\n",
        "def preprocess(input_array, method='zscore'):\n",
        "  if method == 'zscore':\n",
        "    for i in range(input_array.shape[1]):\n",
        "      mean = np.mean(input_array[:, i])\n",
        "      std = np.std(input_array[:, i])\n",
        "      input_array[:, i] = (input_array[:, i] - mean) / std\n",
        "  elif method == 'minmax':\n",
        "    for i in range(input_array.shape[1]):\n",
        "      # range 0 to max\n",
        "      input_array[:, i] = (input_array[:, i] - np.min(input_array[:, i]))\n",
        "      # range 0 to 2\n",
        "      input_array[:, i] /= (np.max(input_array[:, i]) / 2)\n",
        "      # range -1 to 1\n",
        "      input_array[:, i] -= 1\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hCFYVtU06MPR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#use stratified sampling to split train into train/validation\n",
        "#this dataset is actually balanced but still good practice\n",
        "def split(dataset, labels, train_percent=.85):\n",
        "  count = len(dataset)\n",
        "  num_classes = np.max(label) + 1\n",
        "  train = []\n",
        "  train_target = []\n",
        "  validate = []\n",
        "  validate_target = []\n",
        "  for i in range(num_classes):\n",
        "    class_data = np.ravel(np.argwhere(label == i))\n",
        "    np.random.shuffle(class_data)\n",
        "    cutoff = int(len(class_data) * train_percent)\n",
        "    train_idx = class_data[:cutoff]\n",
        "    val_idx = class_data[cutoff:]\n",
        "    train.append(dataset[train_idx])\n",
        "    train_target.append(labels[train_idx])\n",
        "    validate.append(dataset[val_idx])\n",
        "    validate_target.append(labels[val_idx])\n",
        "    \n",
        "  return np.vstack(train), np.hstack(train_target), np.vstack(validate), np.hstack(validate_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RsZmOeyySQq9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#need to one-hot encode labels to map to N output nodes (1 per class)\n",
        "#ie convert each label into a (10,) vector where the relevant column is 1\n",
        "\n",
        "def OHE(input_array, num_classes=10):\n",
        "  output = []\n",
        "  for x in input_array:\n",
        "    output.append(np.zeros((10,)))\n",
        "    output[-1][x] = 1\n",
        "  return np.vstack(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q8KpB-EXchIh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## implemented formulae from here: https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404\n",
        "class InitWeights(object):\n",
        "  def xavier(self, n_in, n_out, uniform=True):\n",
        "    if uniform:\n",
        "      bounds = np.sqrt(6) / (np.sqrt(n_in + n_out))\n",
        "      return self._uniform(n_in, n_out, bounds) \n",
        "    else:\n",
        "      stddev = np.sqrt(2) / (np.sqrt(n_in + n_out))\n",
        "      return self._truncated_normal(n_in, n_out, stddev)\n",
        "    \n",
        "  def he(self, n_in, n_out, uniform=True):\n",
        "    if uniform:\n",
        "      bounds = np.sqrt(2) / (np.sqrt(n_in))\n",
        "      return self._uniform(n_in, n_out, bounds)      \n",
        "    else:\n",
        "      stddev = np.sqrt(6) / (np.sqrt(n_in))\n",
        "      return self._truncated_normal(n_in, n_out, stddev)\n",
        " \n",
        "  def _uniform(self, n_in, n_out, bounds):\n",
        "    W = np.random.uniform(\n",
        "        low=-bounds,\n",
        "        high=bounds,\n",
        "        size=(n_in, n_out)\n",
        "      )\n",
        "    return W\n",
        "  \n",
        "  def _truncated_normal(self, n_in, n_out, stddev):\n",
        "    W = np.random.normal(\n",
        "        loc=0,\n",
        "        scale=stddev,\n",
        "        size=(n_in, n_out)\n",
        "      )\n",
        "    #truncate results - anything > 2 stddev out gets clipped\n",
        "    W[W> 2*stddev] = 2*stddev\n",
        "    W[W<-2*stddev] = -2*stddev\n",
        "    return W\n",
        "  \n",
        "  def __init__(self, init_method=\"xavier\"):\n",
        "    if init_method==\"xavier\":\n",
        "      self.f = self.xavier\n",
        "    elif init_method==\"he\":\n",
        "      self.f = self.he"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "coc2QCMsn63E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calc_MSE(y, y_hat):\n",
        "  error = y-y_hat\n",
        "  return np.mean(np.sum(error**2, axis=1))\n",
        "\n",
        "def labels_from_preds(preds):\n",
        "  return np.argmax(preds, axis=1)\n",
        "\n",
        "def calc_accuracy(labels, target):\n",
        "  return np.sum(labels == target) / len(target)\n",
        "\n",
        "#wasn't sure if we could use a package to shuffle so found this code: https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
        "def shuffle_in_unison(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
        "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
        "    permutation = np.random.permutation(len(a))\n",
        "    for old_index, new_index in enumerate(permutation):\n",
        "        shuffled_a[new_index] = a[old_index]\n",
        "        shuffled_b[new_index] = b[old_index]\n",
        "    return shuffled_a, shuffled_b\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JbRDaYgyBsh8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The function for ReLU\n",
        "\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    x & \\mbox{if } x > 0 \\\\\n",
        "    0 & \\mbox{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "The function for ReLU's derivative\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    1 & \\mbox{if } x > 0 \\\\\n",
        "    0 & \\mbox{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "\n",
        "The function for Leaky ReLU\n",
        "\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    x & \\mbox{if } x > 0 \\\\\n",
        "    0.01x & \\mbox{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "The function for Leaky ReLU's derivative\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    1 & \\mbox{if } x > 0 \\\\\n",
        "    0.01 & \\mbox{otherwise}\n",
        "\\end{cases}$\n"
      ]
    },
    {
      "metadata": {
        "id": "kYXDLyEWGG2r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "class Activation(object):\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def tanh_deriv(self, a):\n",
        "        # a = np.tanh(x)   \n",
        "        return 1.0 - a**2\n",
        "    def logistic(self, x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    def logistic_deriv(self, a):\n",
        "        # a = logistic(x) \n",
        "        return  a * (1 - a )\n",
        "      \n",
        "    def ReLU(self, x):\n",
        "        x[x<0] =0\n",
        "        return x\n",
        "      \n",
        "    def ReLU_deriv(self, a):\n",
        "        der = np.zeros(a.shape)\n",
        "        der[a>0] =1\n",
        "        return der\n",
        "      \n",
        "    def leaky_ReLU(self, x):\n",
        "        x = np.where(x > 0, x, x*0.01)\n",
        "        return x\n",
        "      \n",
        "    def leaky_ReLU_deriv(self, a):\n",
        "        der = np.full(a.shape, 0.01)\n",
        "        der[a>0] =1\n",
        "        return der\n",
        "      \n",
        "    def softmax(self, x):\n",
        "        # apply max normalization to avoid overflow\n",
        "        if len(x.shape) > 1:\n",
        "          x_norm = (x.T - np.max(x, axis=1)).T\n",
        "          return softmax(x_norm, axis=1)\n",
        "        else:\n",
        "          x_norm = x - np.max(x)\n",
        "          return softmax(x_norm)\n",
        "      \n",
        "    def softmax_deriv(self, a):\n",
        "        return np.ones(a.shape)\n",
        "    \n",
        "    def __init__(self,activation='tanh'):\n",
        "        if activation == 'logistic':\n",
        "            self.f = self.logistic\n",
        "            self.f_deriv = self.logistic_deriv\n",
        "        elif activation == 'tanh':\n",
        "            self.f = self.tanh\n",
        "            self.f_deriv = self.tanh_deriv\n",
        "        elif activation == 'relu':\n",
        "            self.f = self.ReLU\n",
        "            self.f_deriv = self.ReLU_deriv\n",
        "        elif activation == 'leaky_relu':\n",
        "            self.f = self.leaky_ReLU\n",
        "            self.f_deriv = self.leaky_ReLU_deriv\n",
        "        elif activation == 'softmax':\n",
        "            self.f = self.softmax\n",
        "            self.f_deriv = self.softmax_deriv\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HbjWqZ24L3Ot",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Loss_function(object):\n",
        "    def MSE(self, y, y_hat):\n",
        "        error = y-y_hat\n",
        "        loss=np.sum(error**2)\n",
        "        return loss\n",
        "      \n",
        "    def Cross_entropy(self, y, y_hat):\n",
        "        return -np.log(y_hat[np.argmax(y)])\n",
        "      \n",
        "    def l2_reg(self, reg_weight, layers, sample_weight):\n",
        "        accum = 0\n",
        "        for layer in layers:\n",
        "          accum += np.sum(np.square(layer.W))\n",
        "          \n",
        "        return accum*reg_weight*sample_weight/2\n",
        "        \n",
        "    def __init__(self,loss='cross_entropy'):\n",
        "        if loss == 'MSE':\n",
        "            self.loss = self.MSE\n",
        "        elif loss == 'cross_entropy':\n",
        "            self.loss = self.Cross_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BeoakGoCGLvb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class HiddenLayer(object):    \n",
        "    def __init__(self,n_in, n_out,\n",
        "                 activation_last_layer='tanh',activation='tanh', W=None, b=None,\n",
        "                init_uniform=True, weight_decay=None, last_layer=False):\n",
        "        \"\"\"\n",
        "        Typical hidden layer of a MLP: units are fully-connected and have\n",
        "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
        "        and the bias vector b is of shape (n_out,).\n",
        "\n",
        "        NOTE : The nonlinearity used here is tanh\n",
        "\n",
        "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
        "\n",
        "        :type n_in: int\n",
        "        :param n_in: dimensionality of input\n",
        "\n",
        "        :type n_out: int\n",
        "        :param n_out: number of hidden units\n",
        "\n",
        "        :type activation: string\n",
        "        :param activation: Non linearity to be applied in the hidden\n",
        "                           layer\n",
        "        :type init_uniform: bool\n",
        "        :param init_uniform: Whether to draw init weights from uniform dist (else normal)\n",
        "        \n",
        "        :type weight_decay: float/None/False\n",
        "        :param weight_decay: Weight to apply to l2 reg loss factor (else none/false)\n",
        "        \"\"\"\n",
        "        self.input=None\n",
        "        self.dropout_cache=None\n",
        "        self.activation=Activation(activation).f\n",
        "        self.last_layer=last_layer\n",
        "        \n",
        "        if activation=='relu':\n",
        "          self.init_weights = InitWeights(\"he\").f\n",
        "        else:\n",
        "          self.init_weights = InitWeights(\"xavier\").f\n",
        "        \n",
        "        # activation deriv of last layer\n",
        "        self.activation_deriv=None\n",
        "        if activation_last_layer:\n",
        "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
        "\n",
        "        if W is not None:\n",
        "          self.W = W\n",
        "        else:\n",
        "          self.W = self.init_weights(n_in, n_out, init_uniform)\n",
        "\n",
        "        if b is not None:\n",
        "          self.b = b\n",
        "        else:\n",
        "          self.b = np.zeros(n_out,)  \n",
        "          \n",
        "        self.weight_decay = weight_decay\n",
        "          \n",
        "        self.grad_W = np.zeros(self.W.shape)\n",
        "        self.grad_b = np.zeros(self.b.shape)\n",
        "        \n",
        "        # create arrays to store the velocity values for momentum calculation\n",
        "        self.vW = np.zeros(self.W.shape)\n",
        "        self.vb = np.zeros(self.b.shape)\n",
        "        \n",
        "        # Create arrays to store the gamma and beta values for batch norm. @\n",
        "        self.gamma = np.zeros(n_out,)\n",
        "        self.beta = np.zeros(n_out,)\n",
        "        \n",
        "        self.grad_gamma = np.zeros(self.gamma.shape,)\n",
        "        self.grad_beta = np.zeros(self.beta.shape,)\n",
        "    \n",
        "    def dropout(self, nodes, probability):\n",
        "        #This distribution decides what nodes will be on or not. We then rescale the on nodes proportionally to the probability that it is off.\n",
        "        \n",
        "        active_nodes = np.random.binomial(1, probability, size=nodes.shape) / probability\n",
        "        output = np.multiply(nodes, active_nodes)\n",
        "        return output, active_nodes\n",
        "    \n",
        "    def forward(self, input, probability):\n",
        "        '''\n",
        "        :type input: numpy.array\n",
        "        :param input: a symbolic tensor of shape (n_in,)\n",
        "        '''\n",
        "        lin_output = np.dot(input, self.W) + self.b\n",
        "        self.output = (\n",
        "            lin_output if self.activation is None\n",
        "            else self.activation(lin_output)\n",
        "        )\n",
        "        self.input=input\n",
        "        \n",
        "        if self.last_layer:\n",
        "            probability=1\n",
        "        \n",
        "        self.output, self.dropout_cache = self.dropout(self.output, probability)\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, delta, output_layer=False, sampleweight=1):\n",
        "        delta *= self.dropout_cache\n",
        "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
        "        \n",
        "        if self.weight_decay:\n",
        "            self.grad_W += self.W * self.weight_decay * sampleweight\n",
        "        self.grad_b = np.sum(delta, axis=0)\n",
        "        if self.activation_deriv:\n",
        "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
        "        return delta\n",
        "      \n",
        "    def forward_BN(self, input, probability):\n",
        "        '''\n",
        "        :type input: numpy.array\n",
        "        :param input: a symbolic tensor of shape (n_in,)\n",
        "        gamma: parameter to be learned\n",
        "        beta: parameter to be learned\n",
        "        '''\n",
        "        lin_output = np.dot(input, self.W) # Removed bias because it is forced to zero when normalizing ~(0,1) @\n",
        "        \n",
        "        # Can apply batch norm before or after activation function: https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/ @\n",
        "        mu = np.mean(lin_output, axis=0) # Calculate the mean of each feature.\n",
        "        var = np.var(lin_output, axis=0) # Calculat the variance of each feature.    \n",
        "                \n",
        "        lin_output_norm = (lin_output - mu) / np.sqrt(var + 1e-8) # Normalise. Note the 1e-8 used incase var = 0.  \n",
        "        lin_output_norm = self.gamma * lin_output_norm + self.beta # Add gamma and beta so the mean and vairaince of distribution can be tuned.\n",
        "        \n",
        "        self.cache = (lin_output, lin_output_norm, mu, var)  # Store values for back propogation.   \n",
        "                      \n",
        "        self.output = (\n",
        "            lin_output_norm if self.activation is None\n",
        "            else self.activation(lin_output_norm)\n",
        "        )\n",
        "        self.input=input # I'm not sure what this does? @\n",
        "        \n",
        "        if self.last_layer:\n",
        "            probability=1\n",
        "        \n",
        "        self.output, self.dropout_cache = self.dropout(self.output, probability)\n",
        "        return self.output                                       \n",
        "\n",
        "    def backward_BN(self, delta, dy_hat, output_layer=False, sampleweight=1):\n",
        "      \n",
        "        # Unpack cache variables\n",
        "        X, X_norm, mu, var = self.cache\n",
        "        \n",
        "        # Define variables to make back prop clearer\n",
        "        n_in, n_dims = X.shape\n",
        "        \n",
        "        X_mu = X - mu\n",
        "        std_inv = 1 / np.sqrt(var + 1e-8)\n",
        "        \n",
        "        # Back prop step by step for clarity. I followed this procedure: https://wiseodd.github.io/techblog/2016/07/04/batchnorm/\n",
        "        # Can also condense into a single calculation like this: https://cthorey.github.io/backpropagation/\n",
        "        dX_norm = delta * self.gamma\n",
        "        dvar = np.sum(dX_norm * X_mu, axis=0) * -0.5 * std_inv**3\n",
        "        dmu = np.sum(dX_norm * -std_inv, axis=0) + dvar * np.mean(-2. * X_mu, axis=0)\n",
        "  \n",
        "        self.grad_W = (dX_norm * std_inv) + (dvar * 2 * X_mu / n_in) + (dmu / n_in)\n",
        "\n",
        "        self.grad_gamma = np.sum(delta * X_norm, axis=0)\n",
        "        self.grad_beta = np.sum(delta, axis=0)            \n",
        "        \n",
        "        if self.weight_decay:\n",
        "          self.grad_W += self.W * self.weight_decay * sampleweight      \n",
        "        \n",
        "        if self.activation_deriv:\n",
        "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
        "        return delta\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_LKgiGhdGQbS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class MLP:\n",
        "    \"\"\"\n",
        "    \"\"\"      \n",
        "    def __init__(self, layers=None, activation=[None,'tanh','tanh'], init_uniform=True, weight_decay=False, from_file=None):\n",
        "        \"\"\"\n",
        "        :param layers: A list containing the number of units in each layer.\n",
        "        Should be at least two values\n",
        "        :param activation: The activation function to be used. Can be\n",
        "        \"logistic\" or \"tanh\"\n",
        "        :param init_uniform: Whether to draw init weights from uniform dist (else normal)\n",
        "        :param weight_decay: lambda for strength of l2 regularization on weights (else False/None for no reg)\n",
        "        :param from_file: a file to load to get pretrained weights. \n",
        "        \"\"\"        \n",
        "        ### initialize layers\n",
        "        self.layers=[]\n",
        "        self.params= {'activation':activation, 'layers':layers, 'weight_decay': weight_decay, 'init_uniform': init_uniform}\n",
        "        \n",
        "        self.es_epochs=None\n",
        "        if from_file:\n",
        "          dumped_model = self._load_model(from_file)\n",
        "          self.params = dumped_model['params']\n",
        "          self.activation=self.params['activation']\n",
        "          layers = self.params['layers']\n",
        "          init_uniform = self.params['init_uniform']\n",
        "          for i in range(len(self.params['layers'])-1):\n",
        "              if i==len(self.params['layers'])-2:\n",
        "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],  \n",
        "                                              W=dumped_model['weights'][i][0], b=dumped_model['weights'][i][1], weight_decay=weight_decay, init_uniform=init_uniform,last_layer=True))\n",
        "              else:\n",
        "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],  \n",
        "                                              W=dumped_model['weights'][i][0], b=dumped_model['weights'][i][1], weight_decay=weight_decay, init_uniform=init_uniform))\n",
        "        else:\n",
        "          self.activation=activation\n",
        "          for i in range(len(layers)-1):\n",
        "              if i==len(layers)-2:\n",
        "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1], weight_decay=weight_decay, init_uniform=init_uniform,last_layer=True))\n",
        "              else:\n",
        "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1], weight_decay=weight_decay, init_uniform=init_uniform))\n",
        "    \n",
        "    def forward(self,input, dropout_p=1):\n",
        "        for layer in self.layers:\n",
        "            output=layer.forward(input, dropout_p)\n",
        "            input=output\n",
        "        return output\n",
        "    \n",
        "    #@ Can combine this with forward above with an if statement. Just wanted to keep separte for now until I know it works.\n",
        "    def Forward_BN(self,input, dropout_p=1):\n",
        "        for layer in self.layers:\n",
        "            output =layer.forward_BN(input, dropout_p)\n",
        "            input=output\n",
        "        return output\n",
        "      \n",
        "    def set_early_stopping(self, validation, validation_labels, num_epochs=10):\n",
        "        # for early stopping\n",
        "        self.validation = validation\n",
        "        self.validation_labels = labels_from_preds(validation_labels)\n",
        "        self.es_epochs = num_epochs\n",
        "      \n",
        "    def calculate_loss(self,y,y_hat):\n",
        "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
        "        # call to loss function\n",
        "        loss=[]\n",
        "        delta=[]\n",
        "        dy_hat=[] # store derivative of output for use in BN_backward @\n",
        "\n",
        "        for i, single_y in enumerate(y):\n",
        "          loss.append(Loss_function('MSE').loss(single_y, y_hat[i]))\n",
        "          error = single_y-y_hat[i]\n",
        "          # calculate the delta of the output layer\n",
        "          delta.append(np.array(-error*activation_deriv(y_hat[i])))\n",
        "        # return loss and delta\n",
        "        loss = np.array(loss)\n",
        "        if self.params['weight_decay']:\n",
        "          loss += Loss_function().l2_reg(self.params['weight_decay'], self.layers, len(y)/self.Xcount)\n",
        "\n",
        "        return loss,np.array(delta)\n",
        "      \n",
        "    def calculate_loss_BN(self,y,y_hat):\n",
        "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
        "        # call to loss function\n",
        "        loss=[]\n",
        "        delta=[]\n",
        "        dy_hat=[] # store derivative of output for use in BN_backward @\n",
        "\n",
        "        for i, single_y in enumerate(y):\n",
        "          loss.append(Loss_function('MSE').loss(single_y, y_hat[i]))\n",
        "          error = single_y-y_hat[i]\n",
        "          # calculate the derivative of the output layer @\n",
        "          dy_hat.append(np.array(activation_deriv(y_hat[i])))\n",
        "          # calculate the delta of the output layer\n",
        "          delta.append(np.array(-error*activation_deriv(y_hat[i])))\n",
        "        # return loss and delta\n",
        "        loss = np.array(loss)\n",
        "        if self.params['weight_decay']:\n",
        "          loss += Loss_function().l2_reg(self.params['weight_decay'], self.layers, len(y)/self.Xcount)\n",
        "\n",
        "        return loss,np.array(delta), np.array(dy_hat) #@\n",
        "        \n",
        "    def backward(self,delta, sampleweight):\n",
        "        delta=self.layers[-1].backward(delta,output_layer=True, sampleweight=sampleweight)\n",
        "        for layer in reversed(self.layers[:-1]):\n",
        "            delta=layer.backward(delta, sampleweight=sampleweight)\n",
        "    \n",
        "    #@ can combine this with backwards above with an if statement. Just wanted to keep separte for now until I know it works.\n",
        "    def Backward_BN(self, delta, dy_hat, sampleweight):\n",
        "        delta =self.layers[-1].backward_BN(delta, dy_hat, output_layer=True, sampleweight=sampleweight)\n",
        "        for layer in reversed(self.layers[:-1]):\n",
        "            delta =layer.backward_BN(delta, dy_hat, sampleweight=sampleweight)                 \n",
        "            \n",
        "    def update(self,lr):\n",
        "        for layer in self.layers:\n",
        "            layer.W -= lr * layer.grad_W\n",
        "            layer.b -= lr * layer.grad_b\n",
        "            \n",
        "    def update_momentum(self, lr, mom):\n",
        "        for layer in self.layers:\n",
        "            layer.vW = mom * layer.vW + lr * layer.grad_W\n",
        "            layer.vb = mom * layer.vb + lr * layer.grad_b\n",
        "            layer.W -= layer.vW\n",
        "            layer.b -= layer.vb \n",
        "            \n",
        "    def update_BN(self,lr):\n",
        "        for layer in self.layers:\n",
        "            layer.W -= lr * layer.grad_W\n",
        "            layer.b -= lr * layer.grad_b\n",
        "            layer.gamma -= lr * layer.grad_gamma\n",
        "            layer.beta -= lr * layer.grad_beta     \n",
        "\n",
        "    def fit(self,X,y,learning_rate=0.1, epochs=100, dropout_p=1):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param X: Input data or features\n",
        "        :param y: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        \"\"\" \n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs)\n",
        "        self.Xcount = len(X)\n",
        "        if self.es_epochs:\n",
        "            validation_acc = np.zeros(epochs)\n",
        "        for k in range(epochs):\n",
        "            #print('epoch', k)\n",
        "            loss=np.zeros(X.shape[0])\n",
        "            for it in range(X.shape[0]):\n",
        "                i=np.random.randint(X.shape[0])\n",
        "                \n",
        "                # forward pass\n",
        "                y_hat = self.forward(X[i], dropout_p)\n",
        "                # backward pass\n",
        "                loss[it],delta=self.calculate_loss([y[i]],[y_hat])\n",
        "                self.backward(delta, 1/self.Xcount)\n",
        "                # update\n",
        "                self.update(learning_rate)\n",
        "            to_return[k] = np.mean(loss)\n",
        "            if not k % 10:\n",
        "              print(\".\", end=\"\")\n",
        "            if self.es_epochs:\n",
        "              preds = self.predict(self.validation)\n",
        "              validation_acc[k] = calc_accuracy(labels_from_preds(preds), self.validation_labels)\n",
        "              if k - np.argmax(validation_acc) > self.es_epochs:\n",
        "                print(\"Haven't improved accuracy on validation set in {} epochs, stopping\".format(self.es_epochs))\n",
        "                break\n",
        "        return to_return\n",
        "      \n",
        "    def fit_mb(self,X,y,mini_batch_size,learning_rate=0.1, epochs=100, dropout_p=1):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param X: Input data or features\n",
        "        :param y: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        :param early_stop: int: stop if haven't improved in this many epochs\n",
        "        \"\"\" \n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs) #array to store values of mean loss for each epoch for plotting later\n",
        "        if self.es_epochs:\n",
        "            validation_acc = np.zeros(epochs)\n",
        "        self.Xcount = len(X)\n",
        "        \n",
        "        for k in range(epochs): #for each epoch\n",
        "            X, y = shuffle_in_unison(X, y) #shuffle the input data and input targets\n",
        "            loss=np.zeros(X.shape[0]) #create array of zeros whose lengths = #samples.\n",
        "            \n",
        "            #partition training data (X, y) into mini-batches\n",
        "            for j in range(0, X.shape[0], mini_batch_size):\n",
        "              X_mini = X[j:j + mini_batch_size]\n",
        "              y_mini = y[j:j + mini_batch_size]\n",
        "              # forward pass\n",
        "              y_hat = self.forward(X_mini, dropout_p) #forward feed the mini_batches to get outputs (y_hat)\n",
        "              \n",
        "              # backwards pass\n",
        "              loss[j:j + mini_batch_size], delta=self.calculate_loss(y[j:j + mini_batch_size], y_hat) #input y and y_hat into calculate_loss. Output = loss and delta\n",
        "              self.backward(delta, mini_batch_size/self.Xcount) #pass delta from calculate_loss to backward.\n",
        "\n",
        "              # update\n",
        "              self.update(learning_rate)\n",
        "            to_return[k] = np.mean(loss) #add mean loss to to_return\n",
        "            if not k % 10:\n",
        "              print(\".\", end=\"\")\n",
        "            if self.es_epochs:\n",
        "              preds = self.predict(self.validation)\n",
        "              validation_acc[k] = calc_accuracy(labels_from_preds(preds), self.validation_labels)\n",
        "              if k - np.argmax(validation_acc) > self.es_epochs:\n",
        "                print(\"Haven't improved accuracy on validation set in {} epochs, stopping\".format(self.es_epochs))\n",
        "                break\n",
        "              \n",
        "        return to_return[:k]\n",
        "      \n",
        "    def fit_mb_BN(self,X,y,mini_batch_size,learning_rate=0.1, epochs=100, dropout_p=1):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param X: Input data or features\n",
        "        :param y: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        :param early_stop: int: stop if haven't improved in this many epochs\n",
        "        \"\"\" \n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs) #array to store values of mean loss for each epoch for plotting later\n",
        "        if self.es_epochs:\n",
        "            validation_acc = np.zeros(epochs)\n",
        "        self.Xcount = len(X)\n",
        "        \n",
        "        for k in range(epochs): #for each epoch\n",
        "            X, y = shuffle_in_unison(X, y) #shuffle the input data and input targets\n",
        "            loss=np.zeros(X.shape[0]) #create array of zeros whose lengths = #samples.\n",
        "            \n",
        "            #partition training data (X, y) into mini-batches\n",
        "            for j in range(0, X.shape[0], mini_batch_size):\n",
        "              X_mini = X[j:j + mini_batch_size]\n",
        "              y_mini = y[j:j + mini_batch_size]\n",
        "              # forward pass\n",
        "              y_hat = self.Forward_BN(X_mini, dropout_p) #forward feed the mini_batches to get outputs (y_hat)\n",
        "              \n",
        "              # backwards pass\n",
        "              loss[j:j + mini_batch_size], delta, dy_hat=self.calculate_loss_BN(y[j:j + mini_batch_size], y_hat) #input y and y_hat into calculate_loss. Output = loss and delta\n",
        "              self.Backward_BN(delta, dy_hat, mini_batch_size/self.Xcount) #pass delta from calculate_loss to backward.\n",
        "\n",
        "              # update\n",
        "              self.update_BN(learning_rate)\n",
        "            to_return[k] = np.mean(loss) #add mean loss to to_return\n",
        "            if not k % 10:\n",
        "              print(\".\", end=\"\")\n",
        "            if self.es_epochs:\n",
        "              preds = self.predict(self.validation)\n",
        "              validation_acc[k] = calc_accuracy(labels_from_preds(preds), self.validation_labels)\n",
        "              if k - np.argmax(validation_acc) > self.es_epochs:\n",
        "                print(\"Haven't improved accuracy on validation set in {} epochs, stopping\".format(self.es_epochs))\n",
        "                break\n",
        "              \n",
        "        return to_return[:k]\n",
        "      \n",
        "    def fit_SGD_momentum(self,X,y,learning_rate=0.1, epochs=100, momentum=0.9, dropout_p=1):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param X: Input data or features\n",
        "        :param y: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        \"\"\" \n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs)\n",
        "        self.Xcount = len(X)\n",
        "        if self.es_epochs:\n",
        "            validation_acc = np.zeros(epochs)\n",
        "        for k in range(epochs):\n",
        "            loss=np.zeros(X.shape[0])\n",
        "            \n",
        "            # loop through training examples\n",
        "            for j in range(X.shape[0]):\n",
        "              i=np.random.randint(X.shape[0])\n",
        "                \n",
        "              # forward pass\n",
        "              y_hat = self.forward(X[i], dropout_p)\n",
        "                \n",
        "              # backward pass\n",
        "              loss[j],delta=self.calculate_loss([y[i]],[y_hat])\n",
        "              self.backward(delta, X.shape[0]/self.Xcount)\n",
        "                \n",
        "              # update\n",
        "              self.update_momentum(learning_rate, momentum)\n",
        "            to_return[k] = np.mean(loss)\n",
        "            if not k % 10:\n",
        "              print(\".\", end=\"\")\n",
        "            if self.es_epochs:\n",
        "              preds = self.predict(self.validation)\n",
        "              validation_acc[k] = calc_accuracy(labels_from_preds(preds), self.validation_labels)\n",
        "              if k - np.argmax(validation_acc) > self.es_epochs:\n",
        "                print(\"Haven't improved accuracy on validation set in {} epochs, stopping\".format(self.es_epochs))\n",
        "                break\n",
        "        return to_return  \n",
        "\n",
        "    def predict(self, x):\n",
        "        x = np.array(x)\n",
        "        output = []\n",
        "        for i in np.arange(x.shape[0]):\n",
        "            output.append(self.forward(x[i,:]))\n",
        "        return np.vstack(output)\n",
        "      \n",
        "\n",
        "    def save_model(self, name):\n",
        "      model = {'params':self.params, 'weights':[]}\n",
        "      for x in self.layers:\n",
        "        model['weights'].append((x.W, x.b))\n",
        "        \n",
        "      with open(model_dir.format(name), 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "        \n",
        "    def _load_model(self, name):\n",
        "      with open(model_dir.format(name), 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2dTOURCJ0Uub",
        "colab_type": "code",
        "outputId": "481ce339-39f4-4fe2-e163-7e562b2d3c33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "cell_type": "code",
      "source": [
        "#Random Search Grid Generator\n",
        "import random\n",
        "\n",
        "no_samples = 10\n",
        "# Initilization (uniform, normal)\n",
        "init = ['uni', 'norm']\n",
        "init_sample = random.choices(init, k = no_samples)\n",
        "\n",
        "# Dropout (1, 0.8, )\n",
        "drop = [1, 0.8, 0.6, 0.4, 0.2]\n",
        "drop_sample = random.choices(drop, k = no_samples)\n",
        "\n",
        "# Activation function (Sigmoid+Tanh, reLU+softmax)\n",
        "af = ['Sig+Tanh', 'reLU+SM']\n",
        "af_sample = random.choices(af, k = no_samples)\n",
        "\n",
        "# Batch norm (Y/N)\n",
        "BN = ['y', 'n']\n",
        "BN_sample = random.choices(BN, k = no_samples)\n",
        "\n",
        "# Weight decay (0, 0.05, 0.1)\n",
        "weight_decay = [0, 0.05, 0.1]\n",
        "wd_sample = random.choices(weight_decay, k = no_samples)\n",
        "\n",
        "# Learning rate (0.01, 0.1, 0.5)\n",
        "LR = [0.01, 0.1, 0.5]\n",
        "LR_sample = random.choices(LR, k = no_samples)\n",
        "\n",
        "random_grid = np.vstack((init_sample, drop_sample, af_sample, BN_sample, wd_sample, LR_sample))\n",
        "print(random_grid)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['uni' 'norm' 'norm' 'uni' 'uni' 'norm' 'norm' 'norm' 'uni' 'uni']\n",
            " ['0.4' '0.4' '0.4' '0.4' '0.2' '0.8' '0.8' '0.4' '0.2' '0.4']\n",
            " ['Sig+Tanh' 'Sig+Tanh' 'Sig+Tanh' 'Sig+Tanh' 'Sig+Tanh' 'reLU+SM'\n",
            "  'Sig+Tanh' 'Sig+Tanh' 'Sig+Tanh' 'reLU+SM']\n",
            " ['n' 'y' 'n' 'y' 'n' 'y' 'y' 'y' 'y' 'y']\n",
            " ['0.0' '0.05' '0.0' '0.0' '0.0' '0.1' '0.1' '0.0' '0.1' '0.0']\n",
            " ['0.01' '0.01' '0.1' '0.1' '0.5' '0.5' '0.01' '0.01' '0.5' '0.01']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DOPAt_9nGd5y",
        "colab_type": "code",
        "outputId": "aee18ad1-25bf-43ea-e875-81a8434659b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "np.seterr(all=\"warn\")\n",
        "np.random.seed(1)\n",
        "procdata = np.copy(data)\n",
        "preprocess(procdata, 'zscore')\n",
        "\n",
        "#split data\n",
        "train, train_target, validate, validate_target = split(procdata, label)\n",
        "#one hot encode targets\n",
        "train_target = OHE(train_target, 10)\n",
        "validate_target = OHE(validate_target, 10)\n",
        "\n",
        "relu = True\n",
        "\n",
        "if relu:\n",
        "  nn = MLP([128,99,69,10], [None,'relu','relu', 'softmax'])\n",
        "  nn.set_early_stopping(validate, validate_target, 10)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb(train, train_target, learning_rate=0.01, epochs=500, mini_batch_size=32)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "else:\n",
        "  nn = MLP([128,60,10], [None,'logistic','tanh'], init_uniform=False, weight_decay=0.5)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb(train, train_target, learning_rate=0.01, epochs=500, mini_batch_size=32)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "..Haven't improved accuracy on validation set in 10 epochs, stopping\n",
            "76.21911907196045s to train\n",
            "loss:0.111792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-U-Uf7i9YX9t",
        "colab_type": "code",
        "outputId": "0604b5a9-23c4-48d5-cdc8-33ca4042994d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "pl.figure(figsize=(15,4))\n",
        "pl.plot(MSE)\n",
        "pl.grid()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3QAAAD8CAYAAAA/m+aTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "oAAnBZZ97anF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ms28tiW02uzz",
        "colab_type": "code",
        "outputId": "960d6453-2a90-42c9-ecec-bd3b18c37c15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#get validation score\n",
        "#nn = load_model(\"tdm1\")\n",
        "preds = nn.predict(validate)\n",
        "\n",
        "loss = calc_MSE(preds, validate_target)\n",
        "calc_accuracy(labels_from_preds(preds), labels_from_preds(validate_target)) # we have to de-OHE the predictions and the target data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8725555555555555"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "Chank-E-im1p",
        "colab_type": "code",
        "outputId": "9b3e36c2-9ffc-4713-fe7e-7426e5430282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "nn.save_model(\"relu.1\")\n",
        "\n",
        "calc_accuracy(labels_from_preds(preds), labels_from_preds(validate_target)) # we have to de-OHE the predictions and the target data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8725555555555555"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "VPIf-BuaBzlC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CauivsamljjX",
        "colab_type": "code",
        "outputId": "246b2d67-e60a-4ee9-d03e-3956e256df0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "cell_type": "code",
      "source": [
        "np.seterr(all=\"warn\")\n",
        "np.random.seed(1)\n",
        "procdata = np.copy(data)\n",
        "preprocess(procdata, 'zscore')\n",
        "\n",
        "#split data\n",
        "train, train_target, validate, validate_target = split(data, label)\n",
        "#one hot encode targets\n",
        "train_target = OHE(train_target, 10)\n",
        "validate_target = OHE(validate_target, 10)\n",
        "second_layer = False\n",
        "relu = False\n",
        "if second_layer:\n",
        "  nn = MLP([128,60,30,10], [None,'logistic','logistic','tanh'])\n",
        "elif relu:\n",
        "  nn = MLP([128,60,10], [None, 'relu', 'relu'], False)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb_BN(train, train_target, 32, learning_rate=0.001, epochs=25, dropout_p=1)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "else:\n",
        "  nn = MLP([128,60,10], [None,'logistic','tanh'], False)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb_BN(train, train_target, 32, learning_rate=0.01, epochs=25, dropout_p=1)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-c41100ed7a8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'logistic'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_mb_BN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}s to train\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss:%f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mMSE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-e030b7038ef3>\u001b[0m in \u001b[0;36mfit_mb_BN\u001b[0;34m(self, X, y, mini_batch_size, learning_rate, epochs, dropout_p)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m               \u001b[0;31m# update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_BN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0mto_return\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#add mean loss to to_return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-e030b7038ef3>\u001b[0m in \u001b[0;36mupdate_BN\u001b[0;34m(self, lr)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_BN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_gamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (128,60) (32,60) (128,60) "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "hYPv5koXCwYk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(train)\n",
        "len(validate)\n",
        "train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XAmMCUzSupSo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pl.figure(figsize=(15,4))\n",
        "pl.plot(MSE)\n",
        "pl.grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oGolRVIfGAcn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HExGAmdDalm0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##1.0 Introduction\n",
        "The experiment task consisted of building a neural network to perform multi-class classification on a supplied dataset without the use of Deep Learning frameworks (e.g. TensorFlow, Caffe, and KERAS). The dataset consisted of 60,000 labeled training samples and 10,000 unlabeled test samples. The structure of the data (e.g. image, video, etc) was unknown. The performance of the neural network was evaluated in terms of the accuracy metric. Various neural network structures and parameters were trailed to maximise speed and accuracy.\n",
        "\n",
        "The objective of building the neural network without Deep Learning frameworks was to gain a comprehensive understanding of the math and mechanics behind neural networks. This is important due to the increasing number of fields where neural networks are the state-of-the-art solution for machine learning.\n",
        "\n",
        "##2.0 Methods\n",
        "###2.1 Equipment\n",
        "The programing language Python3 was used to build the neural network. A **COMPUTER XX@** with the following specifications was used to run the neural network:\n",
        "*   Processor:\n",
        "*   Memory:\n",
        "*   Operating system:\n",
        "\n",
        "###2.2 Preprocessing\n",
        "\n",
        "Preprocessing data before feeding it to a Neural Network is important for many reasons. Firstly, greatly differing magnitudes between different features can lead to the network assigning false importance to the larger features. Preprocessing also speeds up convergence - centering the data can mitigate the vanishing gradient problem on activation functions such as sigmoid and tanh, while normalization can mitigate the exploding gradient problem with ReLU activation passing through huge net values.\n",
        "\n",
        "The method selected was z-score normalization. This is a two-step process - run on each feature independetly. First the mean of each feature is subtracted to centre it on zero; secondly the standard deviation is calculated and the feature divided by the standard deviation. The end result is features with zero mean and standard deviation 1. This ensures most of the variance is within the -1 to 1 range where most activation functions have the largest gradient.\n",
        "\n",
        "The output data was also one-hot encoded to create one column per output class. This maps back to the output of a neural network with one node per output class.\n",
        "\n",
        "###2.3 Weight Initialization\n",
        "\n",
        "The goal of weight initialization is to initialize the layers of the network with small random numbers to ensure that different nodes learn different things. We want to preserve variance through the network, so we examine each layer independently with the goal that outputs of a layer should have the same variance as the inputs. In order to preserve this variance for different activation functions, two methods of weight initialization are employed.\n",
        "\n",
        "Both methods are used to calculate the parameters of a distribution from which weights are selected. Two options are presented to the user - a uniform distribution and a truncated normal distribution. A truncated normal distribution is selected to remove large outliers - any weight with absolute value larger than the maximum of 2 standard deviations has its absolute value set to this limit.  \n",
        "\n",
        "###2.3.1 Xavier Initialization\n",
        "\n",
        "For sigmoid and tanh activation functions, the preservation of variance depends on both the number of inputs and the number of outputs of the layer. Firstly, for the truncated normal case, we want to average the number of inputs and outputs to select parameters for our distribution which are optimal for both.\n",
        "\n",
        "$$Var(W) = \\frac{1}{\\frac{n_{in} + n_{out}}{2}} = \\frac{2}{n_{in} + n_{out}} $$                 \n",
        "$$\\sigma(W) = \\sqrt{\\frac{2}{n_{in} + n_{out}}} $$\n",
        "\n",
        "To select bounds for a uniform distribution, we apply the function\n",
        "$$Var(\\mathit{Uniform(-limit, limit)}) = \\frac{limit^2}{3} $$\n",
        "$$Var(W) = \\frac{2}{n_{in} + n_{out}}  \\Leftrightarrow  limit = \\sqrt{\\frac{6}{n_{in}+n_{out}}} $$\n",
        "\n",
        "\n",
        "###2.3.2 He Initialization\n",
        "\n",
        "For ReLU activation, we encounter the issue that a ReLU unit halves the variance - the variance of negative numbers is nullified. So we must double the above formulae to compensate. One further simplification is that in their original paper, *He. et al* demonstrated that their initialization scheme works for both forward and backward passes, meaning we only need to consider $n_{in}$ and not the average of both. For truncated normal we derive the bounds:\n",
        "$$ Var(W) = \\frac{2}{n_{in}} $$\n",
        "\n",
        "$$ \\sigma(W) = \\sqrt{\\frac{2}{n_{in}}} $$\n",
        "\n",
        "Applying the function for the variance of limits from the Xavier Initialization section, we derive the bounds for uniform He initialization:\n",
        "\n",
        "$$ limit = \\sqrt{\\frac{6}{n_{in}}} $$\n",
        "\n",
        "##2.4 Training\n",
        "\n",
        "### 2.4.1 Gradient Descent\n",
        "Gradient descent is a machine learning optimization method used in deep learning to calculate the model parameters (weights and biases) that minimise the cost function. The gradient descent method invovles iterating through a training dataset and updating weights and baises in accordance with the gradient of error. There are three types of gradient descent. Each uses a different number of training examples to update the model parameters:\n",
        "*   **Batch Gradient Descent** uses the entire training dataset to calculate gradients and update the parameters. Because the entire training dataset is considered parameters updates are smooth however, it can take a long time to make a single update.\n",
        "*   **Stochastic Gradient Descent (SGD)** uses a single randomly selected sample from the training dataset to calculate gradients and update the parameters. Parameter updates are fast but very noisey.\n",
        "*   **Mini-batch Gradient Descent** uses a subset of the training data (e.g. batches of 1000 samples) to calculate gradients and update the parameters. Mini-batch gradient descent is a compromise between batch and stochastic gradient descent. The mini-batch size can be adjusted to find the appropriate balance between fast convergence and noisey updates. \n",
        "\n",
        "Mini-batch gradient descent was inplemented in the neural network due to the flexibility provided by the adjustable mini-batch size. Mini-batch sizes between 32 and 512 samples were used in accordance with the findings of *Keskar et al, 2017.* *Keskar et al* observed that larger batch-sizes resulted in a degradation in the quality of the model, as measured by its ability to generalize. A hybrid of batch gradient descent and stocahastic gradient descent was also implemented. In this method gradients were updated using the same number of samples as the entire training dataset (i.e. 60,000 samples). However, the 60,000 samples were draw stochastically meaning that some sample would have been used multiple times and others not at all. The stochastic approach to batch gradient ensured the neural network was fed data in a different sequence each epoch. This aids the neural networks ability to generalise.\n",
        "\n",
        "### 2.4.2 Stochastic Gradient Descent (SGD) with Momentum\n",
        "Momentum ($v_t$) is an exponentially weighted average of a neural networks gradients. It was used to update the weights ($w_t$) and biases ($b_t$) of the network.\n",
        "\n",
        "$$v_t = \\beta v_{t-1} + \\eta \\nabla_w J(w)$$\n",
        "$$w_t = w_{t-1} - v_t$$\n",
        "\n",
        "Momentum increases for features whose gradients point in the same direction and reduces for features whose gradients change direction. By reducing the fluctuation of gradients convergence is generally sped up. The hyper-parameter $\\beta$ takes a value between 0 - 1 and dictates how many samples are included in the exponential weighted average. A small $\\beta$ value will increase fluctuation because the average is taken over a smaller number of examples. A large $\\beta$ will increase smoothing because the average is taken over a larger number of examples. A $\\beta$ value of 0.9 was implemented. 0.9 is a widely used value for $\\beta$ (*Ruder, 2016*) as it provides a balance between the two extremes of over-smoothing and excessive fluctuations. \n",
        "\n",
        "\n",
        "### 2.4.3 Batch Normalization\n",
        "Batch Normalization is the normalization of a neural network's hidden layers so that the hidden units have standardised mean and variance. It is carried out on training data mini-batches, typically before the activation function is applied *(Ioffe et al, 2015)*. For each mini-batch $(MB)$ the mean $(\\mu)$ and variance $(\\sigma^2)$ is calculated for all features:\n",
        "\n",
        "$$\\mu_{MB} = \\frac{1}{m} \\sum_{i = 1}^{m} x_i$$\n",
        "$$\\sigma_{MB}^2 \\frac{1}{m} \\sum_{i = 1}^{m} (x_i - \\mu_{MB})^2 $$\n",
        "\n",
        "The normalized values of the hidden unit $x_i$ can then be calculated:\n",
        "\n",
        "$$\\tilde{x_i} = \\frac{x_i - \\mu_{MB}}{\\sqrt{\\sigma_{MB}^2+\\epsilon}}$$\n",
        "\n",
        "$\\tilde{x_i}$ has a mean and variance of 0 and 1 respectively. It may be advantageous to alter the mean and variance of $\\tilde{x_i}$ to manipulate its distribution. Learnable hyperparameters $\\gamma$ and $\\beta$ are introduced to $\\tilde{x_i}$ for this purpose:\n",
        "$$\\tilde{x_{i}} = \\gamma\\tilde{x_i} + \\beta $$\n",
        "\n",
        "While debate remains as to why batch normalization is effective *Ioffe et al* offer two reasons why batch normalization speeds up training. Firstly, as reported above, normalizing the input features of a neural network can speed up learning. This is becuase the cost function becomes more symetrical and larger steps (i.e. larger learning rate) can be taken. The same holds true when the hidden unit values are normalized. The second reason is that batch normalization makes weights deeper in the network more robust to changes that take place in earlier layers. During training the distribution of each layers inputs are adjusted as the parameters of the previous layer change. This is known as internal covariate shift. Batch normalization reduces the amount of covariate shift that occurs throughout the networks layers. This is particuarly benefical for the later layers of the network becausethe earlier layers don't shift as much due to their constrained mean and variance. Batch normalisation therefore allows for larger learning rates. It also means that one can be less careful with the initilisation of weights and bias. A secondary benefit of batch normalization is regularization which helps with model overfit.\n",
        "\n",
        "##2.5 Regularization\n",
        "\n",
        "A common issue in machine learning is the tendency to overfit - a model which perfectly describes the training data may fail to generalize when it comes to unseen data. Various methods can be used to regularize the weights of a neural network to ameliorate overfitting.\n",
        "\n",
        "### 2.5.1 Early Stopping\n",
        "\n",
        "The simplest method is an attempt to reduce over-training - early stopping. To achieve this, we split the data into a large training set, and a small validation set. The accuracy on the held-out validation set is measured at the end of each epoch and stored. If the accuracy has not improved for a certain number of epochs, we cease training and return the model.\n",
        "\n",
        "### 2.5.2 Weight Decay\n",
        "\n",
        "Weight decay is essentially a form of $l2$ regularization. An additional term is added to the loss function to suppress larger weights. Used along with back-propagation, this additional term will see the weights tend to decay toward zero when they would otherwise be untouched. \n",
        "\n",
        "The following term is added to the loss function:\n",
        "$$ \\lambda \\sum W_{i}^2 $$\n",
        "\n",
        "During back-propagation, this translates to an additional gradient on each weight of \n",
        "\n",
        "$$ 2W * \\lambda $$\n",
        "\n",
        "where $ \\lambda $ is an additional parameter of the model - the regularization weight.\n",
        "\n",
        "### 2.5.3 Dropout\n",
        "\n",
        "Dropout is a regularisation method patented by Google. It consists of setting the output of neurons in hidden layers to 0 based on a binomial distribution where the probaibility is set as a hyperparameter of the model. The remaining weights are then scaled inversely proportionally to the probaibility set. This is to ensure that the expected magnitude of the neural networks output remains the same as when dropout is not applied (Like during prediction time). \n",
        "\n",
        "It can be said that all neural networks use dropout, however most of them apply a probability of 1 effectivly causing no change to the output. This is what was implemented for this assignment with the default probability being set to 1. \n",
        "\n",
        "Dropout has also been described as a way of performing model averaging without explicitly training different models like what is done with boosting. This description is used because of any given training example a small subset of the neural network learns how to fit the example. This creates a weak learner. Since the subsets of the neural network all share their weights the result is similar to averaging all of their outputs. More on this idea can be found here: http://papers.nips.cc/paper/4878-understanding-dropout.pdf\n",
        "\n",
        "##2.6 Activation Functions\n",
        "\n",
        "Two different activation functions we're implemented as part of this neural network. The Sigmoid and Tanh activation functions were supplied.\n",
        "\n",
        "### 2.6.1 ReLU\n",
        "\n",
        "The ReLU also known as Rectified Linear Unit formulation:\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    x & \\mbox{if } x > 0 \\\\\n",
        "    0 & \\mbox{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "The ReLU activaiton function has gained some popularity in part due to the properties of it's gradient. It's derivative is very quick to calculate and does not get extremely small or large. This is because the formulation for its derivative is: \n",
        "\n",
        "$ f'(x) = \\begin{cases}\n",
        "    1 & \\mbox{if } x > 0 \\\\\n",
        "    0 & \\mbox{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "\n",
        "A variant of ReLU was also implemented. This variant is called Leaky ReLU. The leaky component of ReLU is because it still retains some information when the input value is less than 0 as described below:\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    x & \\mbox{if } x > 0 \\\\\n",
        "    0.01x & \\mbox{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "The derivative for this retains most of the positive properties that the standard ReLU has but this Leaky version results in no dead neurons. This is when a neuron only produces negative values and therefor will only output 0. This means that neurons derivative will also be 0 and it's weights will never change. More on this can be found here: https://arxiv.org/pdf/1505.00853.pdf\n",
        "\n",
        "$ f'(x) = \\begin{cases}\n",
        "    1 & \\mbox{if } x > 0 \\\\\n",
        "    0.01 & \\mbox{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "### 2.6.2 Softmax\n",
        "The Softmax activation function is primarily used on the last layer of a neural network so that its outputs resemble a probability distribution. It achieves this by processing a whole layer as a group instead of individual neurons like the other activation functions used. It does this by rescaling or normalising the outputs of the layer to between 0 and 1. It does this to the eponential of all the values. This is because the result is always postive and the result is closer to the result from argmax than regulkar normalisation is. This is helpful because during prediction time argmax is used but this is not differentiable and so cannot be used in training. \n",
        "\n",
        "The specific implementation of softmax used is from the SciPy library. The source code for this can be found here: https://github.com/scipy/scipy/blob/v1.2.1/scipy/special/_logsumexp.py#L132-L215\n",
        "\n",
        "This particular implementation was used as everything is computed in log space which leads to better numerical stability. \n",
        "\n",
        "##2.7 Cost Function\n",
        "\n",
        "### 2.7.1 Cross Entropy Loss\n"
      ]
    },
    {
      "metadata": {
        "id": "BRDGNA5IPpGY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Louis testing stuff below"
      ]
    },
    {
      "metadata": {
        "id": "WLXWBGImOPVM",
        "colab_type": "code",
        "outputId": "83dcede8-5555-45b0-c89e-e2f3a6bb0503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "np.seterr(all=\"warn\")\n",
        "np.random.seed(1)\n",
        "procdata = np.copy(data)\n",
        "preprocess(procdata, 'zscore')\n",
        "\n",
        "#split data\n",
        "train, train_target, validate, validate_target = split(procdata, label)\n",
        "#one hot encode targets\n",
        "train_target = OHE(train_target, 10)\n",
        "validate_target = OHE(validate_target, 10)\n",
        "\n",
        "\n",
        "nn = MLP([128,105,82, 30,10], [None, 'leaky_relu','leaky_relu','leaky_relu', 'softmax'])\n",
        "nn.set_early_stopping(validate, validate_target, 50)\n",
        "start = time.time()\n",
        "MSE = nn.fit_mb(train, train_target, learning_rate=0.005,mini_batch_size=32, epochs=500, dropout_p=0.8)\n",
        "print(\"{}s to train\".format(time.time() - start))\n",
        "\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: underflow encountered in square\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/special/_logsumexp.py:112: RuntimeWarning: underflow encountered in exp\n",
            "  tmp = np.exp(a - a_max)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/special/_logsumexp.py:215: RuntimeWarning: underflow encountered in exp\n",
            "  return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0C-vQveSFXUZ",
        "colab_type": "code",
        "outputId": "c551c7a9-522d-4936-daca-5180c06c8d7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "preds = nn.predict(validate)\n",
        "calc_accuracy(labels_from_preds(preds), labels_from_preds(validate_target))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8874444444444445"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "6yXjnNSj_bv3",
        "colab_type": "code",
        "outputId": "0ca5998c-a021-47d0-96a2-aeaf935ca73b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "pl.figure(figsize=(15,4))\n",
        "pl.plot(MSE)\n",
        "pl.grid()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAAD8CAYAAAAhUtYgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "P9SnISKQaugu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Ben's testing area"
      ]
    },
    {
      "metadata": {
        "id": "C86iTp53as_q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.seterr(all=\"warn\")\n",
        "np.random.seed(1)\n",
        "procdata = np.copy(data)\n",
        "preprocess(procdata, 'zscore')\n",
        "\n",
        "#split data\n",
        "train, train_target, validate, validate_target = split(procdata, label)\n",
        "#one hot encode targets\n",
        "train_target = OHE(train_target, 10)\n",
        "validate_target = OHE(validate_target, 10)\n",
        "second_layer = True\n",
        "relu = False\n",
        "if second_layer:\n",
        "  nn = MLP([128,69, 99, 10], [None,'relu', 'relu', 'softmax'])\n",
        "elif relu:\n",
        "  nn = MLP([128,69,30,10], [None,'relu', 'relu', 'relu', 'softmax'])\n",
        "  #nn.set_early_stopping(validate, validate_target, 10)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb(train, train_target, learning_rate=0.01, epochs=500, mini_batch_size=32)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "else:\n",
        "  nn = MLP([128,30,10], [None,'relu','softmax'], init_uniform=False, weight_decay=0)\n",
        "  nn.set_early_stopping(validate, validate_target, 10)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb(train, train_target, learning_rate=0.01, epochs=500, mini_batch_size=32)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jHbIDW6iqxfe",
        "colab_type": "code",
        "outputId": "3c3ebaa2-442f-4370-e141-18af16d129ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "np.seterr(all=\"warn\")\n",
        "np.random.seed(1)\n",
        "procdata = np.copy(data)\n",
        "preprocess(procdata, 'zscore')\n",
        "\n",
        "#split data\n",
        "train, train_target, validate, validate_target = split(procdata, label)\n",
        "#one hot encode targets\n",
        "train_target = OHE(train_target, 10)\n",
        "validate_target = OHE(validate_target, 10)\n",
        "second_layer = False\n",
        "relu = False\n",
        "if second_layer:\n",
        "  nn = MLP([128,60,30,10], [None,'logistic','logistic','tanh'])\n",
        "  nn.set_early_stopping(validate, validate_target, 10)\n",
        "elif relu:\n",
        "  nn = MLP([128, 69, 69, 99, 10], [None, 'relu', 'relu', 'relu', 'softmax'], init_uniform=False, weight_decay=0.05)\n",
        "  nn.set_early_stopping(validate, validate_target, 10)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb(train, train_target, learning_rate=0.01,mini_batch_size=32, epochs=20, dropout_p=1)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "else:\n",
        "  nn = MLP([128, 69, 99, 10], [None,'logistic', 'logistic', 'tanh'], init_uniform=True, weight_decay=0.1)\n",
        "  nn.set_early_stopping(validate, validate_target, 100)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_SGD_momentum(train, train_target, learning_rate=0.005, epochs=500, dropout_p=1)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...........Haven't improved accuracy on validation set in 100 epochs, stopping\n",
            "2077.588443517685s to train\n",
            "loss:0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KP_gHGnhcqgm",
        "colab_type": "code",
        "outputId": "ce6a1983-3bae-43ca-bb30-e58b615f9486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "preds = nn.predict(validate)\n",
        "calc_accuracy(labels_from_preds(preds), labels_from_preds(validate_target))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "metadata": {
        "id": "Ap0Rp-bya3-K",
        "colab_type": "code",
        "outputId": "b9410e0e-18da-49c4-fb34-81e65dfc4719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "pl.figure(figsize=(15,4))\n",
        "pl.plot(MSE)\n",
        "pl.grid()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAAD8CAYAAADkIEyxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}